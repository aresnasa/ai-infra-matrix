#!/usr/bin/env python3
"""
JupyterHubä¸Kubernetes GPUé›†æˆç³»ç»Ÿ

é¡¹ç›®æ¦‚è¿°ï¼š
æœ¬è„šæœ¬å®ç°äº†ä¸€ä¸ªå®Œæ•´çš„ç³»ç»Ÿï¼Œå°†JupyterHubé¡¹ç›®é›†æˆåˆ°ä¸»é¡¹ç›®ä¸­ï¼Œ
æ”¯æŒå°†Pythonè„šæœ¬è½¬æ¢ä¸ºKubernetes GPU Jobå¹¶æäº¤åˆ°K8sé›†ç¾¤ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- ğŸ”— JupyterHubé¡¹ç›®é›†æˆå’Œé…ç½®
- ğŸ¯ GPUèµ„æºæŸ¥è¯¢å’Œæ™ºèƒ½è°ƒåº¦
- ğŸ“¦ Pythonè„šæœ¬è‡ªåŠ¨åŒ–å®¹å™¨åŒ–
- ğŸš€ K8s Jobè‡ªåŠ¨æäº¤å’Œç›‘æ§
- ğŸ’¾ NFSå­˜å‚¨é›†æˆå’Œç»“æœç®¡ç†
- ğŸ“Š å®Œæ•´çš„ä»»åŠ¡ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿ

æŠ€æœ¯æ ˆï¼š
- Python: Kuberneteså®¢æˆ·ç«¯ã€JupyterHub API
- Kubernetes: GPUèŠ‚ç‚¹è°ƒåº¦ã€Jobç®¡ç†
- NFS: åˆ†å¸ƒå¼å­˜å‚¨è§£å†³æ–¹æ¡ˆ
- Docker: å®¹å™¨åŒ–è¿è¡Œç¯å¢ƒ
"""

import subprocess
import sys
import os
import time
import json
import uuid
import yaml
from datetime import datetime
from typing import Dict, List, Optional, Any

def install_package(package):
    """å®‰è£…PythonåŒ…"""
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
        print(f"âœ… {package} å®‰è£…æˆåŠŸ")
    except subprocess.CalledProcessError:
        print(f"âŒ {package} å®‰è£…å¤±è´¥")

def setup_environment():
    """ç¯å¢ƒè®¾ç½®å’Œä¾èµ–å®‰è£…"""
    print("ğŸ”§ è®¾ç½®ç¯å¢ƒå’Œå®‰è£…ä¾èµ–...")
    
    # å®‰è£…ä¸»è¦ä¾èµ–
    required_packages = [
        "kubernetes>=24.2.0",
        "pyyaml>=6.0",
        "requests>=2.28.0",
        "aiohttp>=3.8.0",
        "jinja2>=3.1.0",
        "psutil>=5.9.0",
        "docker>=6.0.0"
    ]
    
    for package in required_packages:
        install_package(package)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ.setdefault('AI_INFRA_API_URL', 'http://localhost:8080')
    os.environ.setdefault('JUPYTERHUB_K8S_NAMESPACE', 'jupyterhub-jobs')
    os.environ.setdefault('PYTHON_GPU_IMAGE', 'localhost:5000/jupyterhub-python-gpu:latest')
    os.environ.setdefault('PYTHON_BASE_IMAGE', 'localhost:5000/jupyterhub-python-cpu:latest')
    
    print("âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ")

class JupyterHubK8sIntegration:
    """JupyterHub K8s GPUé›†æˆä¸»ç±»"""
    
    def __init__(self):
        self.api_url = os.environ.get('AI_INFRA_API_URL', 'http://localhost:8080')
        self.namespace = os.environ.get('JUPYTERHUB_K8S_NAMESPACE', 'jupyterhub-jobs')
        self.gpu_image = os.environ.get('PYTHON_GPU_IMAGE')
        self.base_image = os.environ.get('PYTHON_BASE_IMAGE')
        
        # åˆå§‹åŒ–K8så®¢æˆ·ç«¯
        try:
            from kubernetes import client, config
            config.load_incluster_config()  # å®¹å™¨å†…é…ç½®
        except:
            try:
                config.load_kube_config()  # æœ¬åœ°é…ç½®
            except:
                print("âš ï¸ æ— æ³•åŠ è½½Kubernetesé…ç½®")
        
        self.k8s_batch = client.BatchV1Api()
        self.k8s_core = client.CoreV1Api()
    
    def get_gpu_nodes(self) -> List[Dict]:
        """è·å–GPUèŠ‚ç‚¹åˆ—è¡¨"""
        try:
            nodes = self.k8s_core.list_node()
            gpu_nodes = []
            
            for node in nodes.items:
                labels = node.metadata.labels or {}
                
                # æ£€æŸ¥GPUæ ‡ç­¾
                if any(key.startswith('accelerator') for key in labels.keys()):
                    gpu_info = {
                        'name': node.metadata.name,
                        'gpu_type': labels.get('accelerator', 'unknown'),
                        'gpu_count': int(labels.get('gpu-count', '1')),
                        'status': 'Ready' if any(
                            condition.type == 'Ready' and condition.status == 'True'
                            for condition in node.status.conditions
                        ) else 'NotReady'
                    }
                    gpu_nodes.append(gpu_info)
            
            return gpu_nodes
        except Exception as e:
            print(f"âŒ è·å–GPUèŠ‚ç‚¹å¤±è´¥: {e}")
            return []
    
    def create_gpu_job(self, script_content: str, job_name: str = None, 
                      gpu_required: bool = True, gpu_count: int = 1) -> str:
        """åˆ›å»ºGPUä½œä¸š"""
        if not job_name:
            job_name = f"jupyterhub-job-{int(time.time())}"
        
        # é€‰æ‹©é•œåƒ
        image = self.gpu_image if gpu_required else self.base_image
        
        # åˆ›å»ºJobé…ç½®
        job_config = {
            'apiVersion': 'batch/v1',
            'kind': 'Job',
            'metadata': {
                'name': job_name,
                'namespace': self.namespace
            },
            'spec': {
                'template': {
                    'spec': {
                        'containers': [{
                            'name': 'python-executor',
                            'image': image,
                            'command': ['python3', '-c'],
                            'args': [script_content],
                            'resources': {
                                'limits': {
                                    'nvidia.com/gpu': str(gpu_count) if gpu_required else '0',
                                    'memory': '8Gi',
                                    'cpu': '4'
                                },
                                'requests': {
                                    'memory': '4Gi',
                                    'cpu': '2'
                                }
                            },
                            'volumeMounts': [{
                                'name': 'shared-storage',
                                'mountPath': '/shared'
                            }]
                        }],
                        'volumes': [{
                            'name': 'shared-storage',
                            'nfs': {
                                'server': os.environ.get('NFS_SERVER', 'nfs-server'),
                                'path': os.environ.get('NFS_PATH', '/shared')
                            }
                        }],
                        'restartPolicy': 'Never',
                        'nodeSelector': {
                            'accelerator': 'nvidia'
                        } if gpu_required else {}
                    }
                },
                'backoffLimit': 3
            }
        }
        
        try:
            # æäº¤ä½œä¸š
            job = self.k8s_batch.create_namespaced_job(
                namespace=self.namespace,
                body=job_config
            )
            print(f"âœ… ä½œä¸šå·²æäº¤: {job_name}")
            return job_name
        except Exception as e:
            print(f"âŒ ä½œä¸šæäº¤å¤±è´¥: {e}")
            return None
    
    def get_job_status(self, job_name: str) -> Dict:
        """è·å–ä½œä¸šçŠ¶æ€"""
        try:
            job = self.k8s_batch.read_namespaced_job(
                name=job_name,
                namespace=self.namespace
            )
            
            status = {
                'name': job_name,
                'active': job.status.active or 0,
                'succeeded': job.status.succeeded or 0,
                'failed': job.status.failed or 0,
                'completion_time': job.status.completion_time,
                'start_time': job.status.start_time
            }
            
            if status['succeeded'] > 0:
                status['phase'] = 'Succeeded'
            elif status['failed'] > 0:
                status['phase'] = 'Failed'
            elif status['active'] > 0:
                status['phase'] = 'Running'
            else:
                status['phase'] = 'Pending'
            
            return status
        except Exception as e:
            return {'name': job_name, 'phase': 'Unknown', 'error': str(e)}
    
    def get_job_logs(self, job_name: str) -> str:
        """è·å–ä½œä¸šæ—¥å¿—"""
        try:
            # è·å–Jobå¯¹åº”çš„Pod
            pods = self.k8s_core.list_namespaced_pod(
                namespace=self.namespace,
                label_selector=f'job-name={job_name}'
            )
            
            if not pods.items:
                return "æœªæ‰¾åˆ°ç›¸å…³Pod"
            
            pod_name = pods.items[0].metadata.name
            logs = self.k8s_core.read_namespaced_pod_log(
                name=pod_name,
                namespace=self.namespace
            )
            return logs
        except Exception as e:
            return f"è·å–æ—¥å¿—å¤±è´¥: {e}"

def demo_gpu_test():
    """GPUæ€§èƒ½æµ‹è¯•ç¤ºä¾‹"""
    script = '''
import torch
import time
from datetime import datetime

print(f"=== GPUæ€§èƒ½æµ‹è¯• - {datetime.now()} ===")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    device_count = torch.cuda.device_count()
    print(f"GPUæ•°é‡: {device_count}")
    
    for i in range(device_count):
        print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
    
    # æ€§èƒ½æµ‹è¯•
    device = torch.device('cuda')
    size = 1000
    
    a = torch.randn(size, size, device=device)
    b = torch.randn(size, size, device=device)
    
    start_time = time.time()
    c = torch.mm(a, b)
    torch.cuda.synchronize()
    duration = time.time() - start_time
    
    print(f"çŸ©é˜µä¹˜æ³• ({size}x{size}): {duration:.4f}ç§’")
    print("GPUæµ‹è¯•å®Œæˆï¼")
else:
    print("GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæµ‹è¯•")
    import torch
    a = torch.randn(500, 500)
    b = torch.randn(500, 500)
    start_time = time.time()
    c = torch.mm(a, b)
    duration = time.time() - start_time
    print(f"CPUçŸ©é˜µä¹˜æ³•: {duration:.4f}ç§’")
'''
    return script

def demo_ml_training():
    """æœºå™¨å­¦ä¹ è®­ç»ƒç¤ºä¾‹"""
    script = '''
import torch
import torch.nn as nn
import torch.optim as optim
from datetime import datetime

print(f"=== æœºå™¨å­¦ä¹ è®­ç»ƒç¤ºä¾‹ - {datetime.now()} ===")

# ç®€å•çš„ç¥ç»ç½‘ç»œ
class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        return self.fc3(x)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

model = SimpleNet().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
batch_size = 32
x = torch.randn(batch_size, 784).to(device)
y = torch.randint(0, 10, (batch_size,)).to(device)

# è®­ç»ƒå¾ªç¯
for epoch in range(10):
    optimizer.zero_grad()
    outputs = model(x)
    loss = criterion(outputs, y)
    loss.backward()
    optimizer.step()
    
    if epoch % 2 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

print("è®­ç»ƒå®Œæˆï¼")

# ä¿å­˜æ¨¡å‹
torch.save(model.state_dict(), '/shared/simple_model.pth')
print("æ¨¡å‹å·²ä¿å­˜åˆ° /shared/simple_model.pth")
'''
    return script

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ JupyterHub K8s GPUé›†æˆç³»ç»Ÿ")
    print("=" * 50)
    
    # è®¾ç½®ç¯å¢ƒ
    setup_environment()
    
    # åˆå§‹åŒ–é›†æˆç³»ç»Ÿ
    integration = JupyterHubK8sIntegration()
    
    # æ˜¾ç¤ºGPUèŠ‚ç‚¹ä¿¡æ¯
    print("\nğŸ“Š GPUèŠ‚ç‚¹çŠ¶æ€:")
    gpu_nodes = integration.get_gpu_nodes()
    if gpu_nodes:
        for node in gpu_nodes:
            print(f"  {node['name']}: {node['gpu_type']} ({node['gpu_count']}x GPU) - {node['status']}")
    else:
        print("  æœªæ£€æµ‹åˆ°GPUèŠ‚ç‚¹")
    
    # äº¤äº’å¼é€‰æ‹©
    print("\nğŸ¯ é€‰æ‹©æ“ä½œ:")
    print("1. è¿è¡ŒGPUæ€§èƒ½æµ‹è¯•")
    print("2. è¿è¡Œæœºå™¨å­¦ä¹ è®­ç»ƒç¤ºä¾‹")
    print("3. è‡ªå®šä¹‰è„šæœ¬")
    print("4. æŸ¥çœ‹ä½œä¸šçŠ¶æ€")
    
    choice = input("\nè¯·é€‰æ‹©æ“ä½œ (1-4): ").strip()
    
    if choice == '1':
        print("\nğŸ”¥ æäº¤GPUæ€§èƒ½æµ‹è¯•ä½œä¸š...")
        script = demo_gpu_test()
        job_name = integration.create_gpu_job(script, 'gpu-performance-test', gpu_required=True)
        
    elif choice == '2':
        print("\nğŸ¤– æäº¤æœºå™¨å­¦ä¹ è®­ç»ƒä½œä¸š...")
        script = demo_ml_training()
        job_name = integration.create_gpu_job(script, 'ml-training-demo', gpu_required=True)
        
    elif choice == '3':
        print("\nğŸ“ è¯·è¾“å…¥Pythonè„šæœ¬å†…å®¹ (è¾“å…¥ 'END' ç»“æŸ):")
        lines = []
        while True:
            line = input()
            if line.strip() == 'END':
                break
            lines.append(line)
        script = '\n'.join(lines)
        
        job_name = input("ä½œä¸šåç§°: ").strip() or f"custom-job-{int(time.time())}"
        gpu_required = input("éœ€è¦GPU? (y/n): ").strip().lower() == 'y'
        
        job_name = integration.create_gpu_job(script, job_name, gpu_required)
        
    elif choice == '4':
        job_name = input("ä½œä¸šåç§°: ").strip()
        if job_name:
            status = integration.get_job_status(job_name)
            print(f"\nğŸ“‹ ä½œä¸šçŠ¶æ€: {status}")
            
            if input("\næŸ¥çœ‹æ—¥å¿—? (y/n): ").strip().lower() == 'y':
                logs = integration.get_job_logs(job_name)
                print(f"\nğŸ“„ ä½œä¸šæ—¥å¿—:\n{logs}")
    
    if choice in ['1', '2', '3'] and job_name:
        print(f"\nâœ… ä½œä¸šå·²æäº¤: {job_name}")
        
        # ç›‘æ§ä½œä¸šçŠ¶æ€
        if input("ç›‘æ§ä½œä¸šçŠ¶æ€? (y/n): ").strip().lower() == 'y':
            print("\nâ³ ç›‘æ§ä½œä¸šçŠ¶æ€...")
            while True:
                status = integration.get_job_status(job_name)
                print(f"çŠ¶æ€: {status['phase']} | æ´»è·ƒ: {status.get('active', 0)} | "
                      f"æˆåŠŸ: {status.get('succeeded', 0)} | å¤±è´¥: {status.get('failed', 0)}")
                
                if status['phase'] in ['Succeeded', 'Failed']:
                    print(f"\nğŸ ä½œä¸šå®Œæˆ: {status['phase']}")
                    
                    logs = integration.get_job_logs(job_name)
                    print(f"\nğŸ“„ ä½œä¸šæ—¥å¿—:\n{logs}")
                    break
                
                time.sleep(5)

if __name__ == "__main__":
    main()
