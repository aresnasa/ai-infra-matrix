{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a01f155b",
   "metadata": {},
   "source": [
    "# AIåŸºç¡€è®¾æ–½çŸ©é˜µ - K8s GPUä½œä¸šé›†æˆ\n",
    "\n",
    "è¿™ä¸ªNotebookæä¾›äº†ä¸Kubernetes GPUé›†ç¾¤é›†æˆçš„å®Œæ•´åŠŸèƒ½ã€‚\n",
    "\n",
    "## åŠŸèƒ½ç‰¹æ€§\n",
    "- ğŸš€ ä¸€é”®æäº¤Pythonè„šæœ¬åˆ°K8s GPUé›†ç¾¤\n",
    "- ğŸ“Š å®æ—¶GPUèµ„æºç›‘æ§\n",
    "- ğŸ”„ ä½œä¸šçŠ¶æ€è·Ÿè¸ªå’Œæ—¥å¿—æŸ¥çœ‹\n",
    "- ğŸ’¾ NFSå…±äº«å­˜å‚¨é›†æˆ\n",
    "- ğŸ› ï¸ æ™ºèƒ½èµ„æºåˆ†é…å’Œè°ƒåº¦\n",
    "\n",
    "**ä½¿ç”¨å‰è¯·å…ˆè¿è¡Œä¸‹é¢çš„åˆå§‹åŒ–å•å…ƒæ ¼ï¼**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a494752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–ç¯å¢ƒå’Œä¾èµ–\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import importlib.util\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"å®‰è£…PythonåŒ…\"\"\"\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "def check_and_install(packages):\n",
    "    \"\"\"æ£€æŸ¥å¹¶å®‰è£…å¿…è¦çš„åŒ…\"\"\"\n",
    "    for package in packages:\n",
    "        package_name = package.split('==')[0]  # å¤„ç†ç‰ˆæœ¬å·\n",
    "        spec = importlib.util.find_spec(package_name.replace('-', '_'))\n",
    "        if spec is None:\n",
    "            print(f\"æ­£åœ¨å®‰è£… {package}...\")\n",
    "            install_package(package)\n",
    "        else:\n",
    "            print(f\"âœ… {package_name} å·²å®‰è£…\")\n",
    "\n",
    "# å®‰è£…å¿…è¦çš„ä¾èµ–\n",
    "required_packages = [\n",
    "    'requests>=2.25.0',\n",
    "    'ipywidgets>=7.6.0',\n",
    "    'pandas>=1.3.0',\n",
    "    'matplotlib>=3.4.0',\n",
    "    'pyyaml>=6.0'\n",
    "]\n",
    "\n",
    "print(\"ğŸ”§ åˆå§‹åŒ–AIåŸºç¡€è®¾æ–½çŸ©é˜µK8s GPUé›†æˆç¯å¢ƒ...\")\n",
    "check_and_install(required_packages)\n",
    "\n",
    "# è®¾ç½®ç¯å¢ƒå˜é‡\n",
    "os.environ['AI_INFRA_API_URL'] = os.environ.get('AI_INFRA_API_URL', 'http://localhost:8080')\n",
    "os.environ['JUPYTERHUB_K8S_NAMESPACE'] = os.environ.get('JUPYTERHUB_K8S_NAMESPACE', 'jupyterhub-jobs')\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒåˆå§‹åŒ–å®Œæˆï¼\")\n",
    "print(f\"ğŸ“¡ APIåœ°å€: {os.environ['AI_INFRA_API_URL']}\")\n",
    "print(f\"ğŸ¯ K8så‘½åç©ºé—´: {os.environ['JUPYTERHUB_K8S_NAMESPACE']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a5000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K8s GPUé›†æˆå®¢æˆ·ç«¯ç±»\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "class AIInfraK8sClient:\n",
    "    \"\"\"AIåŸºç¡€è®¾æ–½çŸ©é˜µK8s GPUé›†æˆå®¢æˆ·ç«¯\"\"\"\n",
    "    \n",
    "    def __init__(self, api_url: str = None):\n",
    "        self.api_url = api_url or os.environ.get('AI_INFRA_API_URL', 'http://localhost:8080')\n",
    "        self.api_base = f\"{self.api_url}/api/v1/jupyterhub\"\n",
    "        self.session = requests.Session()\n",
    "    \n",
    "    def get_gpu_status(self) -> Dict:\n",
    "        \"\"\"è·å–GPUèµ„æºçŠ¶æ€\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.api_base}/gpu/status\")\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e), \"total_gpus\": 0, \"available_gpus\": 0}\n",
    "    \n",
    "    def find_gpu_nodes(self, gpu_count: int = 1, gpu_type: str = \"\") -> Dict:\n",
    "        \"\"\"æŸ¥æ‰¾é€‚åˆçš„GPUèŠ‚ç‚¹\"\"\"\n",
    "        params = {\"gpu_count\": gpu_count}\n",
    "        if gpu_type:\n",
    "            params[\"gpu_type\"] = gpu_type\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(f\"{self.api_base}/gpu/nodes\", params=params)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e), \"suitable_nodes\": [], \"count\": 0}\n",
    "    \n",
    "    def submit_job(self, name: str, script: str, **kwargs) -> Dict:\n",
    "        \"\"\"æäº¤Pythonè„šæœ¬ä½œä¸š\"\"\"\n",
    "        data = {\n",
    "            \"name\": name,\n",
    "            \"script\": script,\n",
    "            \"requirements\": kwargs.get(\"requirements\", []),\n",
    "            \"gpu_required\": kwargs.get(\"gpu_required\", False),\n",
    "            \"gpu_count\": kwargs.get(\"gpu_count\", 1),\n",
    "            \"gpu_type\": kwargs.get(\"gpu_type\", \"\"),\n",
    "            \"memory_mb\": kwargs.get(\"memory_mb\", 1024),\n",
    "            \"cpu_cores\": kwargs.get(\"cpu_cores\", 1),\n",
    "            \"environment\": kwargs.get(\"environment\", {}),\n",
    "            \"working_dir\": kwargs.get(\"working_dir\", \"/workspace\"),\n",
    "            \"output_path\": kwargs.get(\"output_path\", \"/shared/output\")\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.post(f\"{self.api_base}/jobs/submit\", json=data)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    def get_job_status(self, job_name: str) -> Dict:\n",
    "        \"\"\"è·å–ä½œä¸šçŠ¶æ€\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.api_base}/jobs/{job_name}/status\")\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e), \"status\": \"unknown\"}\n",
    "    \n",
    "    def wait_for_completion(self, job_name: str, timeout: int = 3600, callback=None):\n",
    "        \"\"\"ç­‰å¾…ä½œä¸šå®Œæˆ\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while time.time() - start_time < timeout:\n",
    "            status = self.get_job_status(job_name)\n",
    "            \n",
    "            if callback:\n",
    "                callback(status)\n",
    "            \n",
    "            if status.get(\"status\") in [\"completed\", \"failed\"]:\n",
    "                return status\n",
    "            \n",
    "            time.sleep(5)\n",
    "        \n",
    "        return {\"error\": \"Timeout\", \"status\": \"timeout\"}\n",
    "\n",
    "# åˆ›å»ºå…¨å±€å®¢æˆ·ç«¯å®ä¾‹\n",
    "ai_client = AIInfraK8sClient()\n",
    "print(\"ğŸ¤– AIåŸºç¡€è®¾æ–½K8så®¢æˆ·ç«¯å·²åˆå§‹åŒ–\")\n",
    "print(f\"ğŸ”— è¿æ¥åˆ°: {ai_client.api_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a824f2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUèµ„æºç›‘æ§é¢æ¿\n",
    "def create_gpu_monitor():\n",
    "    \"\"\"åˆ›å»ºGPUèµ„æºç›‘æ§é¢æ¿\"\"\"\n",
    "    \n",
    "    status_output = widgets.Output()\n",
    "    refresh_btn = widgets.Button(description=\"ğŸ”„ åˆ·æ–°\", button_style='info')\n",
    "    auto_refresh = widgets.Checkbox(value=False, description=\"è‡ªåŠ¨åˆ·æ–°(10s)\")\n",
    "    \n",
    "    def update_status():\n",
    "        with status_output:\n",
    "            clear_output(wait=True)\n",
    "            status = ai_client.get_gpu_status()\n",
    "            \n",
    "            if \"error\" in status:\n",
    "                display(HTML(f\"<div style='color: red;'>âŒ è¿æ¥é”™è¯¯: {status['error']}</div>\"))\n",
    "                return\n",
    "            \n",
    "            # åˆ›å»ºçŠ¶æ€æ˜¾ç¤º\n",
    "            html = f\"\"\"\n",
    "            <div style='border: 1px solid #ddd; padding: 15px; border-radius: 5px; background: #f9f9f9;'>\n",
    "                <h3>ğŸ¯ GPUé›†ç¾¤èµ„æºçŠ¶æ€</h3>\n",
    "                <div style='margin: 10px 0;'>\n",
    "                    <span style='font-weight: bold; color: #2e7d32;'>æ€»GPUæ•°é‡:</span> {status.get('total_gpus', 0)}<br>\n",
    "                    <span style='font-weight: bold; color: #1976d2;'>å¯ç”¨GPU:</span> {status.get('available_gpus', 0)}<br>\n",
    "                    <span style='font-weight: bold; color: #d32f2f;'>å·²ç”¨GPU:</span> {status.get('used_gpus', 0)}<br>\n",
    "                    <span style='font-weight: bold;'>æ›´æ–°æ—¶é—´:</span> {status.get('last_updated', 'N/A')}\n",
    "                </div>\n",
    "            \"\"\"\n",
    "            \n",
    "            # GPUèŠ‚ç‚¹è¯¦æƒ…\n",
    "            if 'gpu_nodes' in status and status['gpu_nodes']:\n",
    "                html += \"<h4>ğŸ“‹ GPUèŠ‚ç‚¹è¯¦æƒ…:</h4><table style='width: 100%; border-collapse: collapse;'>\"\n",
    "                html += \"<tr style='background: #e3f2fd;'><th>èŠ‚ç‚¹åç§°</th><th>GPUç±»å‹</th><th>æ€»æ•°</th><th>å¯ç”¨</th><th>çŠ¶æ€</th></tr>\"\n",
    "                \n",
    "                for node in status['gpu_nodes']:\n",
    "                    status_icon = \"âœ…\" if node.get('schedulable') else \"âŒ\"\n",
    "                    html += f\"\"\"\n",
    "                    <tr style='border-bottom: 1px solid #ddd;'>\n",
    "                        <td style='padding: 5px;'>{node.get('node_name', 'N/A')}</td>\n",
    "                        <td style='padding: 5px;'>{node.get('gpu_type', 'N/A')}</td>\n",
    "                        <td style='padding: 5px;'>{node.get('gpu_count', 0)}</td>\n",
    "                        <td style='padding: 5px;'>{node.get('available_gpus', 0)}</td>\n",
    "                        <td style='padding: 5px;'>{status_icon}</td>\n",
    "                    </tr>\n",
    "                    \"\"\"\n",
    "                html += \"</table>\"\n",
    "            else:\n",
    "                html += \"<p style='color: orange;'>âš ï¸ æœªæ‰¾åˆ°GPUèŠ‚ç‚¹æˆ–èŠ‚ç‚¹ä¿¡æ¯ä¸å¯ç”¨</p>\"\n",
    "            \n",
    "            html += \"</div>\"\n",
    "            display(HTML(html))\n",
    "    \n",
    "    def on_refresh_click(b):\n",
    "        update_status()\n",
    "    \n",
    "    refresh_btn.on_click(on_refresh_click)\n",
    "    \n",
    "    # è‡ªåŠ¨åˆ·æ–°é€»è¾‘\n",
    "    import threading\n",
    "    def auto_refresh_worker():\n",
    "        while auto_refresh.value:\n",
    "            update_status()\n",
    "            time.sleep(10)\n",
    "    \n",
    "    def on_auto_refresh_change(change):\n",
    "        if change['new']:\n",
    "            thread = threading.Thread(target=auto_refresh_worker, daemon=True)\n",
    "            thread.start()\n",
    "    \n",
    "    auto_refresh.observe(on_auto_refresh_change, names='value')\n",
    "    \n",
    "    # åˆå§‹åŠ è½½\n",
    "    update_status()\n",
    "    \n",
    "    return widgets.VBox([\n",
    "        widgets.HBox([refresh_btn, auto_refresh]),\n",
    "        status_output\n",
    "    ])\n",
    "\n",
    "# æ˜¾ç¤ºGPUç›‘æ§é¢æ¿\n",
    "print(\"ğŸ“Š åˆ›å»ºGPUèµ„æºç›‘æ§é¢æ¿...\")\n",
    "gpu_monitor = create_gpu_monitor()\n",
    "display(gpu_monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443bcc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½œä¸šæäº¤ç•Œé¢\n",
    "def create_job_submission_panel():\n",
    "    \"\"\"åˆ›å»ºä½œä¸šæäº¤é¢æ¿\"\"\"\n",
    "    \n",
    "    # è¡¨å•æ§ä»¶\n",
    "    job_name = widgets.Text(placeholder=\"è¾“å…¥ä½œä¸šåç§°\", description=\"ä½œä¸šåç§°:\")\n",
    "    script_area = widgets.Textarea(\n",
    "        placeholder=\"è¾“å…¥Pythonè„šæœ¬ä»£ç ...\",\n",
    "        description=\"Pythonè„šæœ¬:\",\n",
    "        rows=10,\n",
    "        layout=widgets.Layout(width='100%')\n",
    "    )\n",
    "    \n",
    "    requirements = widgets.Text(\n",
    "        placeholder=\"ä¾‹å¦‚: torch pandas numpy\",\n",
    "        description=\"ä¾èµ–åŒ…:\"\n",
    "    )\n",
    "    \n",
    "    gpu_required = widgets.Checkbox(value=False, description=\"éœ€è¦GPU\")\n",
    "    gpu_count = widgets.IntSlider(value=1, min=1, max=8, description=\"GPUæ•°é‡:\")\n",
    "    gpu_type = widgets.Text(placeholder=\"ä¾‹å¦‚: rtx4090\", description=\"GPUç±»å‹:\")\n",
    "    \n",
    "    memory_mb = widgets.IntSlider(value=2048, min=512, max=16384, step=512, description=\"å†…å­˜(MB):\")\n",
    "    cpu_cores = widgets.IntSlider(value=2, min=1, max=16, description=\"CPUæ ¸å¿ƒ:\")\n",
    "    \n",
    "    submit_btn = widgets.Button(description=\"ğŸš€ æäº¤ä½œä¸š\", button_style='success')\n",
    "    output_area = widgets.Output()\n",
    "    \n",
    "    # é¢„è®¾ç¤ºä¾‹\n",
    "    examples = widgets.Dropdown(\n",
    "        options=[\n",
    "            ('é€‰æ‹©ç¤ºä¾‹...', ''),\n",
    "            ('GPUæ£€æµ‹è„šæœ¬', 'gpu_test'),\n",
    "            ('æ•°æ®åˆ†æè„šæœ¬', 'data_analysis'),\n",
    "            ('æœºå™¨å­¦ä¹ è®­ç»ƒ', 'ml_training')\n",
    "        ],\n",
    "        description=\"ç¤ºä¾‹:\"\n",
    "    )\n",
    "    \n",
    "    def load_example(change):\n",
    "        if change['new'] == 'gpu_test':\n",
    "            job_name.value = \"GPUæ£€æµ‹ä»»åŠ¡\"\n",
    "            script_area.value = \"\"\"\n",
    "import torch\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"å¼€å§‹GPUæ£€æµ‹ - {datetime.now()}\")\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"GPUè®¾å¤‡æ•°é‡: {device_count}\")\n",
    "    \n",
    "    for i in range(device_count):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"  å†…å­˜: {props.total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    # ç®€å•GPUè®¡ç®—æµ‹è¯•\n",
    "    print(\"\\næ‰§è¡ŒGPUçŸ©é˜µè®¡ç®—æµ‹è¯•...\")\n",
    "    device = torch.device('cuda')\n",
    "    a = torch.randn(1000, 1000, device=device)\n",
    "    b = torch.randn(1000, 1000, device=device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    c = torch.mm(a, b)\n",
    "    compute_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"çŸ©é˜µä¹˜æ³•è€—æ—¶: {compute_time:.4f}ç§’\")\n",
    "    print(f\"ç»“æœæ£€æŸ¥: mean={c.mean().item():.4f}\")\n",
    "else:\n",
    "    print(\"GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè®¡ç®—\")\n",
    "\n",
    "print(f\"ä»»åŠ¡å®Œæˆ - {datetime.now()}\")\n",
    "\"\"\"\n",
    "            requirements.value = \"torch\"\n",
    "            gpu_required.value = True\n",
    "            memory_mb.value = 4096\n",
    "            \n",
    "        elif change['new'] == 'data_analysis':\n",
    "            job_name.value = \"æ•°æ®åˆ†æä»»åŠ¡\"\n",
    "            script_area.value = \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "print(f\"å¼€å§‹æ•°æ®åˆ†æ - {datetime.now()}\")\n",
    "\n",
    "# ç”Ÿæˆç¤ºä¾‹æ•°æ®\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'A': np.random.randn(1000),\n",
    "    'B': np.random.randn(1000) * 2 + 1,\n",
    "    'C': np.random.choice(['X', 'Y', 'Z'], 1000),\n",
    "    'D': np.random.uniform(0, 100, 1000)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"æ•°æ®å½¢çŠ¶: {df.shape}\")\n",
    "print(\"\\næ•°æ®ç»Ÿè®¡:\")\n",
    "print(df.describe())\n",
    "\n",
    "# åˆ†ç»„åˆ†æ\n",
    "print(\"\\næŒ‰ç±»åˆ«åˆ†ç»„åˆ†æ:\")\n",
    "grouped = df.groupby('C').agg({\n",
    "    'A': ['mean', 'std'],\n",
    "    'B': ['mean', 'std'],\n",
    "    'D': ['mean', 'max', 'min']\n",
    "})\n",
    "print(grouped)\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'data_shape': df.shape,\n",
    "    'summary_stats': df.describe().to_dict(),\n",
    "    'category_analysis': grouped.to_dict()\n",
    "}\n",
    "\n",
    "with open('/shared/data_analysis_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\nç»“æœå·²ä¿å­˜åˆ° /shared/data_analysis_results.json\")\n",
    "print(f\"åˆ†æå®Œæˆ - {datetime.now()}\")\n",
    "\"\"\"\n",
    "            requirements.value = \"pandas numpy matplotlib\"\n",
    "            gpu_required.value = False\n",
    "            memory_mb.value = 2048\n",
    "            \n",
    "        elif change['new'] == 'ml_training':\n",
    "            job_name.value = \"æœºå™¨å­¦ä¹ è®­ç»ƒ\"\n",
    "            script_area.value = \"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "print(f\"å¼€å§‹æœºå™¨å­¦ä¹ è®­ç»ƒ - {datetime.now()}\")\n",
    "\n",
    "# è®¾ç½®è®¾å¤‡\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "\n",
    "# ç”Ÿæˆç¤ºä¾‹æ•°æ®\n",
    "torch.manual_seed(42)\n",
    "X = torch.randn(1000, 10, device=device)\n",
    "y = torch.sum(X[:, :5], dim=1) + torch.randn(1000, device=device) * 0.1\n",
    "\n",
    "# å®šä¹‰ç®€å•ç¥ç»ç½‘ç»œ\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(10, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x).squeeze()\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "model = SimpleNet().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"å¼€å§‹è®­ç»ƒ...\")\n",
    "losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/100], Loss: {loss.item():.6f}\")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'device': str(device),\n",
    "    'final_loss': losses[-1],\n",
    "    'training_losses': losses,\n",
    "    'model_parameters': sum(p.numel() for p in model.parameters())\n",
    "}\n",
    "\n",
    "with open('/shared/ml_training_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nè®­ç»ƒå®Œæˆ - æœ€ç»ˆæŸå¤±: {losses[-1]:.6f}\")\n",
    "print(\"ç»“æœå·²ä¿å­˜åˆ° /shared/ml_training_results.json\")\n",
    "print(f\"è®­ç»ƒç»“æŸ - {datetime.now()}\")\n",
    "\"\"\"\n",
    "            requirements.value = \"torch\"\n",
    "            gpu_required.value = True\n",
    "            memory_mb.value = 4096\n",
    "    \n",
    "    examples.observe(load_example, names='value')\n",
    "    \n",
    "    def on_submit_click(b):\n",
    "        with output_area:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            if not job_name.value or not script_area.value:\n",
    "                display(HTML(\"<div style='color: red;'>âŒ è¯·å¡«å†™ä½œä¸šåç§°å’Œè„šæœ¬å†…å®¹</div>\"))\n",
    "                return\n",
    "            \n",
    "            print(\"ğŸ“¤ æ­£åœ¨æäº¤ä½œä¸š...\")\n",
    "            \n",
    "            # å‡†å¤‡ä½œä¸šå‚æ•°\n",
    "            job_params = {\n",
    "                \"requirements\": [pkg.strip() for pkg in requirements.value.split() if pkg.strip()],\n",
    "                \"gpu_required\": gpu_required.value,\n",
    "                \"gpu_count\": gpu_count.value if gpu_required.value else 0,\n",
    "                \"gpu_type\": gpu_type.value,\n",
    "                \"memory_mb\": memory_mb.value,\n",
    "                \"cpu_cores\": cpu_cores.value\n",
    "            }\n",
    "            \n",
    "            # æäº¤ä½œä¸š\n",
    "            result = ai_client.submit_job(job_name.value, script_area.value, **job_params)\n",
    "            \n",
    "            if \"error\" in result:\n",
    "                display(HTML(f\"<div style='color: red;'>âŒ æäº¤å¤±è´¥: {result['error']}</div>\"))\n",
    "                return\n",
    "            \n",
    "            job_id = result.get('job_id')\n",
    "            job_name_submitted = result.get('job_name')\n",
    "            \n",
    "            display(HTML(f\"\"\"\n",
    "            <div style='border: 1px solid #4caf50; padding: 15px; border-radius: 5px; background: #e8f5e8;'>\n",
    "                <h3>âœ… ä½œä¸šæäº¤æˆåŠŸ!</h3>\n",
    "                <p><strong>ä½œä¸šID:</strong> {job_id}</p>\n",
    "                <p><strong>ä½œä¸šåç§°:</strong> {job_name_submitted}</p>\n",
    "                <p><strong>çŠ¶æ€:</strong> {result.get('status', 'submitted')}</p>\n",
    "            </div>\n",
    "            \"\"\"))\n",
    "            \n",
    "            # å¼€å§‹ç›‘æ§ä½œä¸šçŠ¶æ€\n",
    "            print(\"\\nğŸ” ç›‘æ§ä½œä¸šçŠ¶æ€...\")\n",
    "            \n",
    "            def status_callback(status):\n",
    "                current_status = status.get('status', 'unknown')\n",
    "                print(f\"â° {datetime.now().strftime('%H:%M:%S')} - çŠ¶æ€: {current_status}\")\n",
    "            \n",
    "            final_status = ai_client.wait_for_completion(job_name_submitted, callback=status_callback)\n",
    "            \n",
    "            if final_status.get('status') == 'completed':\n",
    "                display(HTML(\"<div style='color: green; font-weight: bold;'>ğŸ‰ ä½œä¸šæ‰§è¡Œå®Œæˆ!</div>\"))\n",
    "            elif final_status.get('status') == 'failed':\n",
    "                display(HTML(f\"<div style='color: red;'>âŒ ä½œä¸šæ‰§è¡Œå¤±è´¥: {final_status.get('error_message', 'æœªçŸ¥é”™è¯¯')}</div>\"))\n",
    "            else:\n",
    "                display(HTML(f\"<div style='color: orange;'>â³ ä½œä¸šçŠ¶æ€: {final_status.get('status', 'unknown')}</div>\"))\n",
    "    \n",
    "    submit_btn.on_click(on_submit_click)\n",
    "    \n",
    "    # å¸ƒå±€\n",
    "    form_layout = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>ğŸš€ K8s GPUä½œä¸šæäº¤</h3>\"),\n",
    "        examples,\n",
    "        job_name,\n",
    "        script_area,\n",
    "        requirements,\n",
    "        widgets.HBox([gpu_required, gpu_count, gpu_type]),\n",
    "        widgets.HBox([memory_mb, cpu_cores]),\n",
    "        submit_btn,\n",
    "        output_area\n",
    "    ])\n",
    "    \n",
    "    return form_layout\n",
    "\n",
    "# æ˜¾ç¤ºä½œä¸šæäº¤é¢æ¿\n",
    "print(\"ğŸ“ åˆ›å»ºä½œä¸šæäº¤é¢æ¿...\")\n",
    "job_panel = create_job_submission_panel()\n",
    "display(job_panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6584bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿«é€Ÿæ“ä½œå·¥å…·æ \n",
    "def create_quick_actions():\n",
    "    \"\"\"åˆ›å»ºå¿«é€Ÿæ“ä½œå·¥å…·æ \"\"\"\n",
    "    \n",
    "    health_btn = widgets.Button(description=\"ğŸ©º å¥åº·æ£€æŸ¥\", button_style='info')\n",
    "    gpu_test_btn = widgets.Button(description=\"ğŸ”¬ GPUæµ‹è¯•\", button_style='warning')\n",
    "    examples_btn = widgets.Button(description=\"ğŸ“š æŸ¥çœ‹ç¤ºä¾‹\", button_style='primary')\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def health_check(b):\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            print(\"ğŸ©º æ‰§è¡Œå¥åº·æ£€æŸ¥...\")\n",
    "            \n",
    "            try:\n",
    "                response = ai_client.session.get(f\"{ai_client.api_url}/api/v1/jupyterhub/health\")\n",
    "                if response.status_code == 200:\n",
    "                    print(\"âœ… APIæœåŠ¡æ­£å¸¸\")\n",
    "                else:\n",
    "                    print(f\"âŒ APIæœåŠ¡å¼‚å¸¸: {response.status_code}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ è¿æ¥å¤±è´¥: {e}\")\n",
    "            \n",
    "            # æ£€æŸ¥GPUçŠ¶æ€\n",
    "            gpu_status = ai_client.get_gpu_status()\n",
    "            if \"error\" not in gpu_status:\n",
    "                print(f\"âœ… GPUé›†ç¾¤è¿æ¥æ­£å¸¸ - å¯ç”¨GPU: {gpu_status.get('available_gpus', 0)}\")\n",
    "            else:\n",
    "                print(f\"âŒ GPUé›†ç¾¤è¿æ¥å¼‚å¸¸: {gpu_status['error']}\")\n",
    "    \n",
    "    def quick_gpu_test(b):\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            print(\"ğŸ”¬ å¯åŠ¨GPUå¿«é€Ÿæµ‹è¯•...\")\n",
    "            \n",
    "            test_script = \"\"\"\n",
    "import torch\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPUæ•°é‡: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "print(\"GPUæµ‹è¯•å®Œæˆ\")\n",
    "\"\"\"\n",
    "            \n",
    "            result = ai_client.submit_job(\n",
    "                \"GPUå¿«é€Ÿæµ‹è¯•\",\n",
    "                test_script,\n",
    "                requirements=[\"torch\"],\n",
    "                gpu_required=True,\n",
    "                memory_mb=1024\n",
    "            )\n",
    "            \n",
    "            if \"error\" in result:\n",
    "                print(f\"âŒ æµ‹è¯•æäº¤å¤±è´¥: {result['error']}\")\n",
    "            else:\n",
    "                print(f\"âœ… æµ‹è¯•ä½œä¸šå·²æäº¤: {result.get('job_name')}\")\n",
    "                print(\"è¯·ç­‰å¾…å‡ åˆ†é’ŸæŸ¥çœ‹ç»“æœ...\")\n",
    "    \n",
    "    def show_examples(b):\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(\"\"\"\n",
    "            <div style='border: 1px solid #ddd; padding: 15px; border-radius: 5px;'>\n",
    "                <h3>ğŸ“š ä½¿ç”¨ç¤ºä¾‹</h3>\n",
    "                \n",
    "                <h4>1. GPUæ£€æµ‹è„šæœ¬</h4>\n",
    "                <p>æ£€æµ‹GPUè®¾å¤‡ä¿¡æ¯å’Œæ‰§è¡Œç®€å•çš„çŸ©é˜µè®¡ç®—æµ‹è¯•</p>\n",
    "                <ul>\n",
    "                    <li>éœ€è¦GPU: âœ…</li>\n",
    "                    <li>ä¾èµ–: torch</li>\n",
    "                    <li>å†…å­˜: 4GB</li>\n",
    "                </ul>\n",
    "                \n",
    "                <h4>2. æ•°æ®åˆ†æè„šæœ¬</h4>\n",
    "                <p>ä½¿ç”¨pandaså’Œnumpyè¿›è¡Œæ•°æ®å¤„ç†å’Œç»Ÿè®¡åˆ†æ</p>\n",
    "                <ul>\n",
    "                    <li>éœ€è¦GPU: âŒ</li>\n",
    "                    <li>ä¾èµ–: pandas numpy matplotlib</li>\n",
    "                    <li>å†…å­˜: 2GB</li>\n",
    "                </ul>\n",
    "                \n",
    "                <h4>3. æœºå™¨å­¦ä¹ è®­ç»ƒ</h4>\n",
    "                <p>ä½¿ç”¨PyTorchè®­ç»ƒç®€å•çš„ç¥ç»ç½‘ç»œæ¨¡å‹</p>\n",
    "                <ul>\n",
    "                    <li>éœ€è¦GPU: âœ…</li>\n",
    "                    <li>ä¾èµ–: torch</li>\n",
    "                    <li>å†…å­˜: 4GB</li>\n",
    "                </ul>\n",
    "                \n",
    "                <h4>ğŸ’¡ ä½¿ç”¨æç¤º</h4>\n",
    "                <ul>\n",
    "                    <li>åœ¨ä½œä¸šæäº¤é¢æ¿é€‰æ‹©ç›¸åº”ç¤ºä¾‹è‡ªåŠ¨å¡«å……ä»£ç </li>\n",
    "                    <li>å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹è„šæœ¬å†…å®¹å’Œèµ„æºé…ç½®</li>\n",
    "                    <li>ä½œä¸šç»“æœä¼šä¿å­˜åˆ°/sharedç›®å½•ä¸‹</li>\n",
    "                    <li>ä½¿ç”¨GPUç›‘æ§é¢æ¿æŸ¥çœ‹é›†ç¾¤èµ„æºçŠ¶æ€</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "            \"\"\"))\n",
    "    \n",
    "    health_btn.on_click(health_check)\n",
    "    gpu_test_btn.on_click(quick_gpu_test)\n",
    "    examples_btn.on_click(show_examples)\n",
    "    \n",
    "    return widgets.VBox([\n",
    "        widgets.HTML(\"<h3>âš¡ å¿«é€Ÿæ“ä½œ</h3>\"),\n",
    "        widgets.HBox([health_btn, gpu_test_btn, examples_btn]),\n",
    "        output\n",
    "    ])\n",
    "\n",
    "# æ˜¾ç¤ºå¿«é€Ÿæ“ä½œé¢æ¿\n",
    "print(\"âš¡ åˆ›å»ºå¿«é€Ÿæ“ä½œå·¥å…·æ ...\")\n",
    "quick_actions = create_quick_actions()\n",
    "display(quick_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71afdfdb",
   "metadata": {},
   "source": [
    "## ğŸ¯ ä½¿ç”¨æŒ‡å—\n",
    "\n",
    "### 1. ç›‘æ§GPUèµ„æº\n",
    "- ä½¿ç”¨ä¸Šé¢çš„GPUç›‘æ§é¢æ¿æŸ¥çœ‹é›†ç¾¤çŠ¶æ€\n",
    "- å¯ä»¥å¼€å¯è‡ªåŠ¨åˆ·æ–°åŠŸèƒ½\n",
    "- æŸ¥çœ‹å„ä¸ªGPUèŠ‚ç‚¹çš„è¯¦ç»†ä¿¡æ¯\n",
    "\n",
    "### 2. æäº¤ä½œä¸š\n",
    "- åœ¨ä½œä¸šæäº¤é¢æ¿å¡«å†™ä½œä¸šä¿¡æ¯\n",
    "- å¯ä»¥é€‰æ‹©é¢„è®¾ç¤ºä¾‹å¿«é€Ÿå¼€å§‹\n",
    "- æ ¹æ®éœ€è¦é…ç½®GPUå’Œå†…å­˜èµ„æº\n",
    "- æäº¤åè‡ªåŠ¨ç›‘æ§ä½œä¸šçŠ¶æ€\n",
    "\n",
    "### 3. å¿«é€Ÿæ“ä½œ\n",
    "- å¥åº·æ£€æŸ¥: éªŒè¯APIæœåŠ¡å’ŒGPUé›†ç¾¤è¿æ¥\n",
    "- GPUæµ‹è¯•: å¿«é€Ÿæäº¤GPUæ£€æµ‹ä½œä¸š\n",
    "- æŸ¥çœ‹ç¤ºä¾‹: äº†è§£ä¸åŒç±»å‹çš„ä½œä¸šç¤ºä¾‹\n",
    "\n",
    "### 4. ç»“æœæŸ¥çœ‹\n",
    "- ä½œä¸šç»“æœä¿å­˜åœ¨`/shared`ç›®å½•ä¸‹\n",
    "- å¯ä»¥é€šè¿‡æ–‡ä»¶æµè§ˆå™¨æŸ¥çœ‹è¾“å‡ºæ–‡ä»¶\n",
    "- JSONæ ¼å¼çš„ç»“æœå¯ä»¥ç›´æ¥åŠ è½½åˆ†æ\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ“ æ³¨æ„äº‹é¡¹:**\n",
    "- ç¡®ä¿åç«¯APIæœåŠ¡æ­£åœ¨è¿è¡Œ\n",
    "- GPUä½œä¸šéœ€è¦é›†ç¾¤æœ‰å¯ç”¨çš„GPUèŠ‚ç‚¹\n",
    "- å¤§å‹ä½œä¸šè¯·åˆç†é…ç½®å†…å­˜å’ŒCPUèµ„æº\n",
    "- å¯ä»¥é€šè¿‡kubectlå‘½ä»¤æŸ¥çœ‹K8sé›†ç¾¤ä¸­çš„ä½œä¸šçŠ¶æ€\n",
    "\n",
    "**ğŸ”— ç›¸å…³èµ„æº:**\n",
    "- APIæ–‡æ¡£: http://localhost:8080/swagger/\n",
    "- GPUç›‘æ§: ä½¿ç”¨ä¸Šé¢çš„ç›‘æ§é¢æ¿\n",
    "- ä½œä¸šæ—¥å¿—: `kubectl logs -f job/<job-name> -n jupyterhub-jobs`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
