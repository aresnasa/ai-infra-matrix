{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c720da",
   "metadata": {},
   "source": [
    "# JupyterHubä¸Kubernetes GPUé›†æˆç³»ç»Ÿ\n",
    "\n",
    "## é¡¹ç›®æ¦‚è¿°\n",
    "\n",
    "æœ¬notebookå®ç°äº†ä¸€ä¸ªå®Œæ•´çš„ç³»ç»Ÿï¼Œå°†JupyterHubé¡¹ç›®é›†æˆåˆ°ä¸»é¡¹ç›®ä¸­ï¼Œæ”¯æŒå°†Pythonè„šæœ¬è½¬æ¢ä¸ºKubernetes GPU Jobå¹¶æäº¤åˆ°K8sé›†ç¾¤ã€‚\n",
    "\n",
    "### ä¸»è¦åŠŸèƒ½\n",
    "- ğŸ”— JupyterHubé¡¹ç›®é›†æˆå’Œé…ç½®\n",
    "- ğŸ¯ GPUèµ„æºæŸ¥è¯¢å’Œæ™ºèƒ½è°ƒåº¦\n",
    "- ğŸ“¦ Pythonè„šæœ¬è‡ªåŠ¨åŒ–å®¹å™¨åŒ–\n",
    "- ğŸš€ K8s Jobè‡ªåŠ¨æäº¤å’Œç›‘æ§\n",
    "- ğŸ’¾ NFSå­˜å‚¨é›†æˆå’Œç»“æœç®¡ç†\n",
    "- ğŸ“Š å®Œæ•´çš„ä»»åŠ¡ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿ\n",
    "\n",
    "### æŠ€æœ¯æ ˆ\n",
    "- **Python**: Kuberneteså®¢æˆ·ç«¯ã€JupyterHub API\n",
    "- **Kubernetes**: GPUèŠ‚ç‚¹è°ƒåº¦ã€Jobç®¡ç†\n",
    "- **NFS**: åˆ†å¸ƒå¼å­˜å‚¨è§£å†³æ–¹æ¡ˆ\n",
    "- **Docker**: å®¹å™¨åŒ–è¿è¡Œç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e122f3a",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒè®¾ç½®å’Œä¾èµ–å®‰è£…\n",
    "\n",
    "å®‰è£…å¿…è¦çš„Pythonåº“ï¼Œé…ç½®åŸºç¡€ç¯å¢ƒå˜é‡å’Œå¸¸é‡å®šä¹‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95000bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"å®‰è£…PythonåŒ…\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"âœ… {package} å®‰è£…æˆåŠŸ\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"âŒ {package} å®‰è£…å¤±è´¥\")\n",
    "\n",
    "# å®‰è£…ä¸»è¦ä¾èµ–\n",
    "required_packages = [\n",
    "    \"kubernetes>=24.2.0\",\n",
    "    \"pyyaml>=6.0\",\n",
    "    \"requests>=2.28.0\",\n",
    "    \"aiohttp>=3.8.0\",\n",
    "    \"jinja2>=3.1.0\",\n",
    "    \"psutil>=5.9.0\",\n",
    "    \"docker>=6.0.0\"\n",
    "]\n",
    "\n",
    "for package in required_packages:\n",
    "    install_package(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08e9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥æ‰€éœ€çš„åº“\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import logging\n",
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Kubernetes ç›¸å…³å¯¼å…¥\n",
    "from kubernetes import client, config\n",
    "from kubernetes.client.rest import ApiException\n",
    "\n",
    "# é…ç½®æ—¥å¿—\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler('jupyterhub-k8s-integration.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ç¯å¢ƒå˜é‡å’Œå¸¸é‡é…ç½®\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"ç³»ç»Ÿé…ç½®ç±»\"\"\"\n",
    "    # K8sé›†ç¾¤é…ç½®\n",
    "    K8S_NAMESPACE: str = \"jupyterhub-gpu\"\n",
    "    K8S_CONFIG_PATH: str = os.getenv(\"KUBECONFIG\", \"~/.kube/config\")\n",
    "    \n",
    "    # NFSå­˜å‚¨é…ç½®\n",
    "    NFS_SERVER: str = os.getenv(\"NFS_SERVER\", \"nfs-server.default.svc.cluster.local\")\n",
    "    NFS_PATH: str = os.getenv(\"NFS_PATH\", \"/shared/jupyterhub\")\n",
    "    NFS_MOUNT_PATH: str = \"/shared\"\n",
    "    \n",
    "    # JupyterHubé…ç½®\n",
    "    JUPYTERHUB_API_URL: str = os.getenv(\"JUPYTERHUB_API_URL\", \"http://localhost:8000/hub/api\")\n",
    "    JUPYTERHUB_API_TOKEN: str = os.getenv(\"JUPYTERHUB_API_TOKEN\", \"\")\n",
    "    \n",
    "    # GPUé…ç½®\n",
    "    GPU_VENDOR_NVIDIA: str = \"nvidia.com/gpu\"\n",
    "    GPU_VENDOR_AMD: str = \"amd.com/gpu\"\n",
    "    GPU_MEMORY_LIMIT: str = \"8Gi\"\n",
    "    \n",
    "    # ä½œä¸šé…ç½®\n",
    "    JOB_IMAGE_BASE: str = \"jupyter/scipy-notebook:latest\"\n",
    "    JOB_TIMEOUT_SECONDS: int = 3600  # 1å°æ—¶è¶…æ—¶\n",
    "    JOB_RETRY_LIMIT: int = 3\n",
    "    \n",
    "    # ç¬¬ä¸‰æ–¹é¡¹ç›®è·¯å¾„\n",
    "    THIRD_PARTY_PATH: str = \"../third-party\"\n",
    "    JUPYTERHUB_PATH: str = f\"{THIRD_PARTY_PATH}/jupyterhub\"\n",
    "\n",
    "# å…¨å±€é…ç½®å®ä¾‹\n",
    "config_instance = Config()\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒé…ç½®åˆå§‹åŒ–å®Œæˆ\")\n",
    "print(f\"ğŸ“ JupyterHubè·¯å¾„: {config_instance.JUPYTERHUB_PATH}\")\n",
    "print(f\"ğŸŒ K8så‘½åç©ºé—´: {config_instance.K8S_NAMESPACE}\")\n",
    "print(f\"ğŸ’¾ NFSæœåŠ¡å™¨: {config_instance.NFS_SERVER}\")\n",
    "print(f\"ğŸ¯ GPUå†…å­˜é™åˆ¶: {config_instance.GPU_MEMORY_LIMIT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2c6dc9",
   "metadata": {},
   "source": [
    "## 2. JupyterHubé¡¹ç›®é›†æˆé…ç½®\n",
    "\n",
    "è¯»å–third_partyä¸‹çš„JupyterHubé¡¹ç›®é…ç½®ï¼Œè§£æé¡¹ç›®ç»“æ„ï¼Œå»ºç«‹ä¸ä¸»é¡¹ç›®çš„é›†æˆæ¥å£ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f4030",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JupyterHubIntegrator:\n",
    "    \"\"\"JupyterHubé›†æˆç®¡ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.jupyterhub_path = Path(config.JUPYTERHUB_PATH)\n",
    "        self.scripts_registry = {}\n",
    "        \n",
    "    def scan_jupyterhub_project(self) -> Dict[str, Any]:\n",
    "        \"\"\"æ‰«æJupyterHubé¡¹ç›®ç»“æ„\"\"\"\n",
    "        project_info = {\n",
    "            \"path\": str(self.jupyterhub_path),\n",
    "            \"exists\": self.jupyterhub_path.exists(),\n",
    "            \"config_files\": [],\n",
    "            \"example_notebooks\": [],\n",
    "            \"python_modules\": [],\n",
    "            \"spawner_configs\": []\n",
    "        }\n",
    "        \n",
    "        if not project_info[\"exists\"]:\n",
    "            logger.warning(f\"JupyterHubé¡¹ç›®è·¯å¾„ä¸å­˜åœ¨: {self.jupyterhub_path}\")\n",
    "            return project_info\n",
    "            \n",
    "        # æ‰«æé…ç½®æ–‡ä»¶\n",
    "        for pattern in [\"*.py\", \"*.yaml\", \"*.yml\", \"*.json\"]:\n",
    "            project_info[\"config_files\"].extend(\n",
    "                list(self.jupyterhub_path.glob(pattern))\n",
    "            )\n",
    "            \n",
    "        # æ‰«æç¤ºä¾‹notebooks\n",
    "        examples_path = self.jupyterhub_path / \"examples\"\n",
    "        if examples_path.exists():\n",
    "            project_info[\"example_notebooks\"] = list(examples_path.glob(\"**/*.ipynb\"))\n",
    "            \n",
    "        # æ‰«æPythonæ¨¡å—\n",
    "        jupyterhub_module = self.jupyterhub_path / \"jupyterhub\"\n",
    "        if jupyterhub_module.exists():\n",
    "            project_info[\"python_modules\"] = list(jupyterhub_module.glob(\"**/*.py\"))\n",
    "            \n",
    "        # æŸ¥æ‰¾spawneré…ç½®\n",
    "        spawner_files = list(self.jupyterhub_path.glob(\"**/spawner*.py\"))\n",
    "        project_info[\"spawner_configs\"] = spawner_files\n",
    "        \n",
    "        logger.info(f\"âœ… æ‰«æåˆ° {len(project_info['config_files'])} ä¸ªé…ç½®æ–‡ä»¶\")\n",
    "        logger.info(f\"âœ… æ‰«æåˆ° {len(project_info['example_notebooks'])} ä¸ªç¤ºä¾‹notebook\")\n",
    "        logger.info(f\"âœ… æ‰«æåˆ° {len(project_info['python_modules'])} ä¸ªPythonæ¨¡å—\")\n",
    "        \n",
    "        return project_info\n",
    "    \n",
    "    def extract_notebook_scripts(self, notebook_path: Path) -> List[Dict[str, Any]]:\n",
    "        \"\"\"ä»notebookä¸­æå–Pythonè„šæœ¬\"\"\"\n",
    "        scripts = []\n",
    "        \n",
    "        try:\n",
    "            with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "                notebook_content = json.load(f)\n",
    "                \n",
    "            for i, cell in enumerate(notebook_content.get('cells', [])):\n",
    "                if cell.get('cell_type') == 'code':\n",
    "                    source = ''.join(cell.get('source', []))\n",
    "                    if source.strip():\n",
    "                        scripts.append({\n",
    "                            \"cell_index\": i,\n",
    "                            \"source\": source,\n",
    "                            \"notebook\": str(notebook_path),\n",
    "                            \"has_gpu_code\": self._detect_gpu_usage(source),\n",
    "                            \"estimated_resources\": self._estimate_resources(source)\n",
    "                        })\n",
    "                        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"è§£ænotebookå¤±è´¥ {notebook_path}: {e}\")\n",
    "            \n",
    "        return scripts\n",
    "    \n",
    "    def _detect_gpu_usage(self, code: str) -> bool:\n",
    "        \"\"\"æ£€æµ‹ä»£ç æ˜¯å¦ä½¿ç”¨GPU\"\"\"\n",
    "        gpu_indicators = [\n",
    "            \"cuda\", \"gpu\", \"torch.cuda\", \"tf.config.experimental\",\n",
    "            \"tensorflow-gpu\", \"cupy\", \"numba.cuda\", \".to('cuda')\",\n",
    "            \"device='cuda'\", \"nvidia-smi\"\n",
    "        ]\n",
    "        \n",
    "        return any(indicator in code.lower() for indicator in gpu_indicators)\n",
    "    \n",
    "    def _estimate_resources(self, code: str) -> Dict[str, str]:\n",
    "        \"\"\"ä¼°ç®—ä»£ç èµ„æºéœ€æ±‚\"\"\"\n",
    "        resources = {\n",
    "            \"cpu\": \"1\",\n",
    "            \"memory\": \"2Gi\",\n",
    "            \"gpu\": \"0\"\n",
    "        }\n",
    "        \n",
    "        # åŸºäºä»£ç ç‰¹å¾ä¼°ç®—èµ„æºéœ€æ±‚\n",
    "        if \"torch\" in code or \"tensorflow\" in code:\n",
    "            resources[\"memory\"] = \"4Gi\"\n",
    "            \n",
    "        if self._detect_gpu_usage(code):\n",
    "            resources[\"gpu\"] = \"1\"\n",
    "            resources[\"memory\"] = \"8Gi\"\n",
    "            \n",
    "        if \"multiprocessing\" in code or \"concurrent\" in code:\n",
    "            resources[\"cpu\"] = \"2\"\n",
    "            \n",
    "        # æ£€æµ‹å¤§æ•°æ®å¤„ç†\n",
    "        if any(word in code for word in [\"pandas\", \"numpy\", \"large\", \"big\"]):\n",
    "            resources[\"memory\"] = \"8Gi\"\n",
    "            \n",
    "        return resources\n",
    "\n",
    "# åˆå§‹åŒ–JupyterHubé›†æˆå™¨\n",
    "integrator = JupyterHubIntegrator(config_instance)\n",
    "\n",
    "# æ‰«æJupyterHubé¡¹ç›®\n",
    "project_info = integrator.scan_jupyterhub_project()\n",
    "\n",
    "print(\"ğŸ“Š JupyterHubé¡¹ç›®æ‰«æç»“æœ:\")\n",
    "for key, value in project_info.items():\n",
    "    if isinstance(value, list):\n",
    "        print(f\"  {key}: {len(value)} é¡¹\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# å¦‚æœå­˜åœ¨ç¤ºä¾‹notebooksï¼Œè§£æç¬¬ä¸€ä¸ªä½œä¸ºæ¼”ç¤º\n",
    "if project_info[\"example_notebooks\"]:\n",
    "    first_notebook = project_info[\"example_notebooks\"][0]\n",
    "    scripts = integrator.extract_notebook_scripts(first_notebook)\n",
    "    print(f\"\\nğŸ” è§£æç¤ºä¾‹notebook: {first_notebook.name}\")\n",
    "    print(f\"  æå–åˆ° {len(scripts)} ä¸ªä»£ç ç‰‡æ®µ\")\n",
    "    \n",
    "    for i, script in enumerate(scripts[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª\n",
    "        print(f\"  ç‰‡æ®µ {i+1}: GPUéœ€æ±‚={script['has_gpu_code']}, èµ„æº={script['estimated_resources']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb92b2",
   "metadata": {},
   "source": [
    "## 3. Kuberneteså®¢æˆ·ç«¯åˆå§‹åŒ–\n",
    "\n",
    "åˆå§‹åŒ–Kubernetes Pythonå®¢æˆ·ç«¯ï¼Œé…ç½®é›†ç¾¤è¿æ¥å‚æ•°ï¼ŒéªŒè¯APIè®¿é—®æƒé™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10404b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KubernetesManager:\n",
    "    \"\"\"Kubernetesé›†ç¾¤ç®¡ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.v1_core = None\n",
    "        self.v1_batch = None\n",
    "        self.v1_apps = None\n",
    "        self.custom_api = None\n",
    "        self.cluster_info = {}\n",
    "        \n",
    "    def initialize_client(self) -> bool:\n",
    "        \"\"\"åˆå§‹åŒ–Kuberneteså®¢æˆ·ç«¯\"\"\"\n",
    "        try:\n",
    "            # å°è¯•åŠ è½½é›†ç¾¤å†…é…ç½®\n",
    "            try:\n",
    "                config.load_incluster_config()\n",
    "                logger.info(\"âœ… ä½¿ç”¨é›†ç¾¤å†…é…ç½®è¿æ¥K8s\")\n",
    "            except:\n",
    "                # fallbackåˆ°æœ¬åœ°é…ç½®æ–‡ä»¶\n",
    "                config.load_kube_config(config_file=self.config.K8S_CONFIG_PATH)\n",
    "                logger.info(\"âœ… ä½¿ç”¨æœ¬åœ°é…ç½®æ–‡ä»¶è¿æ¥K8s\")\n",
    "            \n",
    "            # åˆå§‹åŒ–APIå®¢æˆ·ç«¯\n",
    "            self.v1_core = client.CoreV1Api()\n",
    "            self.v1_batch = client.BatchV1Api()\n",
    "            self.v1_apps = client.AppsV1Api()\n",
    "            self.custom_api = client.CustomObjectsApi()\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ K8så®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def verify_cluster_access(self) -> Dict[str, Any]:\n",
    "        \"\"\"éªŒè¯é›†ç¾¤è®¿é—®æƒé™å¹¶è·å–é›†ç¾¤ä¿¡æ¯\"\"\"\n",
    "        cluster_status = {\n",
    "            \"connected\": False,\n",
    "            \"namespace_exists\": False,\n",
    "            \"permissions\": {},\n",
    "            \"cluster_version\": \"\",\n",
    "            \"node_count\": 0,\n",
    "            \"gpu_nodes\": 0\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # æ£€æŸ¥APIæœåŠ¡å™¨è¿æ¥\n",
    "            version = self.v1_core.get_api_resources()\n",
    "            cluster_status[\"connected\"] = True\n",
    "            logger.info(\"âœ… K8s APIæœåŠ¡å™¨è¿æ¥æˆåŠŸ\")\n",
    "            \n",
    "            # è·å–é›†ç¾¤ç‰ˆæœ¬\n",
    "            version_info = self.v1_core.get_api_versions()\n",
    "            cluster_status[\"cluster_version\"] = str(version_info.versions)\n",
    "            \n",
    "            # æ£€æŸ¥å‘½åç©ºé—´\n",
    "            try:\n",
    "                self.v1_core.read_namespace(name=self.config.K8S_NAMESPACE)\n",
    "                cluster_status[\"namespace_exists\"] = True\n",
    "                logger.info(f\"âœ… å‘½åç©ºé—´ {self.config.K8S_NAMESPACE} å­˜åœ¨\")\n",
    "            except ApiException as e:\n",
    "                if e.status == 404:\n",
    "                    logger.warning(f\"âš ï¸ å‘½åç©ºé—´ {self.config.K8S_NAMESPACE} ä¸å­˜åœ¨ï¼Œå°†åˆ›å»º\")\n",
    "                    self._create_namespace()\n",
    "                    cluster_status[\"namespace_exists\"] = True\n",
    "                    \n",
    "            # è·å–èŠ‚ç‚¹ä¿¡æ¯\n",
    "            nodes = self.v1_core.list_node()\n",
    "            cluster_status[\"node_count\"] = len(nodes.items)\n",
    "            \n",
    "            # ç»Ÿè®¡GPUèŠ‚ç‚¹\n",
    "            gpu_nodes = 0\n",
    "            for node in nodes.items:\n",
    "                if self._node_has_gpu(node):\n",
    "                    gpu_nodes += 1\n",
    "                    \n",
    "            cluster_status[\"gpu_nodes\"] = gpu_nodes\n",
    "            \n",
    "            # æ£€æŸ¥æƒé™\n",
    "            cluster_status[\"permissions\"] = self._check_permissions()\n",
    "            \n",
    "            self.cluster_info = cluster_status\n",
    "            logger.info(f\"ğŸ“Š é›†ç¾¤ä¿¡æ¯: {cluster_status['node_count']} èŠ‚ç‚¹, {cluster_status['gpu_nodes']} GPUèŠ‚ç‚¹\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ é›†ç¾¤è®¿é—®éªŒè¯å¤±è´¥: {e}\")\n",
    "            \n",
    "        return cluster_status\n",
    "    \n",
    "    def _create_namespace(self):\n",
    "        \"\"\"åˆ›å»ºå‘½åç©ºé—´\"\"\"\n",
    "        try:\n",
    "            namespace = client.V1Namespace(\n",
    "                metadata=client.V1ObjectMeta(\n",
    "                    name=self.config.K8S_NAMESPACE,\n",
    "                    labels={\n",
    "                        \"name\": self.config.K8S_NAMESPACE,\n",
    "                        \"purpose\": \"jupyterhub-gpu-jobs\"\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            self.v1_core.create_namespace(body=namespace)\n",
    "            logger.info(f\"âœ… åˆ›å»ºå‘½åç©ºé—´: {self.config.K8S_NAMESPACE}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ åˆ›å»ºå‘½åç©ºé—´å¤±è´¥: {e}\")\n",
    "    \n",
    "    def _node_has_gpu(self, node) -> bool:\n",
    "        \"\"\"æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦æœ‰GPU\"\"\"\n",
    "        allocatable = node.status.allocatable or {}\n",
    "        capacity = node.status.capacity or {}\n",
    "        \n",
    "        gpu_resources = [\n",
    "            self.config.GPU_VENDOR_NVIDIA,\n",
    "            self.config.GPU_VENDOR_AMD,\n",
    "            \"nvidia.com/gpu\",\n",
    "            \"amd.com/gpu\"\n",
    "        ]\n",
    "        \n",
    "        for gpu_resource in gpu_resources:\n",
    "            if gpu_resource in allocatable or gpu_resource in capacity:\n",
    "                return True\n",
    "                \n",
    "        return False\n",
    "    \n",
    "    def _check_permissions(self) -> Dict[str, bool]:\n",
    "        \"\"\"æ£€æŸ¥å¿…è¦çš„K8sæƒé™\"\"\"\n",
    "        permissions = {\n",
    "            \"list_nodes\": False,\n",
    "            \"create_jobs\": False,\n",
    "            \"list_jobs\": False,\n",
    "            \"delete_jobs\": False,\n",
    "            \"create_pods\": False,\n",
    "            \"list_pods\": False\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # æ£€æŸ¥èŠ‚ç‚¹åˆ—è¡¨æƒé™\n",
    "            self.v1_core.list_node(limit=1)\n",
    "            permissions[\"list_nodes\"] = True\n",
    "            \n",
    "            # æ£€æŸ¥Jobç›¸å…³æƒé™\n",
    "            self.v1_batch.list_namespaced_job(namespace=self.config.K8S_NAMESPACE, limit=1)\n",
    "            permissions[\"list_jobs\"] = True\n",
    "            permissions[\"create_jobs\"] = True  # å¦‚æœèƒ½listé€šå¸¸ä¹Ÿèƒ½create\n",
    "            \n",
    "            # æ£€æŸ¥Podç›¸å…³æƒé™\n",
    "            self.v1_core.list_namespaced_pod(namespace=self.config.K8S_NAMESPACE, limit=1)\n",
    "            permissions[\"list_pods\"] = True\n",
    "            permissions[\"create_pods\"] = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"æƒé™æ£€æŸ¥æ—¶å‡ºç°é”™è¯¯: {e}\")\n",
    "            \n",
    "        return permissions\n",
    "\n",
    "# åˆå§‹åŒ–Kubernetesç®¡ç†å™¨\n",
    "k8s_manager = KubernetesManager(config_instance)\n",
    "\n",
    "# åˆå§‹åŒ–å®¢æˆ·ç«¯å¹¶éªŒè¯è¿æ¥\n",
    "print(\"ğŸ”— æ­£åœ¨åˆå§‹åŒ–Kubernetesè¿æ¥...\")\n",
    "if k8s_manager.initialize_client():\n",
    "    print(\"âœ… Kuberneteså®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ\")\n",
    "    \n",
    "    # éªŒè¯é›†ç¾¤è®¿é—®\n",
    "    print(\"ğŸ” æ­£åœ¨éªŒè¯é›†ç¾¤è®¿é—®æƒé™...\")\n",
    "    cluster_status = k8s_manager.verify_cluster_access()\n",
    "    \n",
    "    print(\"\\nğŸ“Š é›†ç¾¤çŠ¶æ€:\")\n",
    "    for key, value in cluster_status.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "        \n",
    "    if cluster_status[\"connected\"] and cluster_status[\"gpu_nodes\"] > 0:\n",
    "        print(f\"\\nğŸ‰ é›†ç¾¤è¿æ¥æˆåŠŸï¼å‘ç° {cluster_status['gpu_nodes']} ä¸ªGPUèŠ‚ç‚¹\")\n",
    "    elif cluster_status[\"connected\"]:\n",
    "        print(\"\\nâš ï¸ é›†ç¾¤è¿æ¥æˆåŠŸï¼Œä½†æœªå‘ç°GPUèŠ‚ç‚¹\")\n",
    "    else:\n",
    "        print(\"\\nâŒ é›†ç¾¤è¿æ¥å¤±è´¥\")\n",
    "else:\n",
    "    print(\"âŒ Kuberneteså®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6607993",
   "metadata": {},
   "source": [
    "## 4. GPUèµ„æºæŸ¥è¯¢å’ŒèŠ‚ç‚¹æ ‡ç­¾æ£€æµ‹\n",
    "\n",
    "å®ç°æŸ¥è¯¢K8sé›†ç¾¤GPUèµ„æºçš„å‡½æ•°ï¼Œæ£€æµ‹èŠ‚ç‚¹æ ‡ç­¾å’Œæ±¡ç‚¹ï¼Œè·å–GPUå‹å·å’Œå¯ç”¨æ€§ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5585fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPUNodeInfo:\n",
    "    \"\"\"GPUèŠ‚ç‚¹ä¿¡æ¯\"\"\"\n",
    "    name: str\n",
    "    gpu_count: int\n",
    "    gpu_type: str\n",
    "    gpu_memory: str\n",
    "    available_gpus: int\n",
    "    labels: Dict[str, str]\n",
    "    taints: List[Dict[str, str]]\n",
    "    schedulable: bool\n",
    "\n",
    "@dataclass \n",
    "class GPUResourceStatus:\n",
    "    \"\"\"GPUèµ„æºçŠ¶æ€\"\"\"\n",
    "    total_gpus: int\n",
    "    available_gpus: int\n",
    "    used_gpus: int\n",
    "    gpu_nodes: List[GPUNodeInfo]\n",
    "    \n",
    "class GPUResourceManager:\n",
    "    \"\"\"GPUèµ„æºç®¡ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, k8s_manager: KubernetesManager):\n",
    "        self.k8s_manager = k8s_manager\n",
    "        self.gpu_cache = {}\n",
    "        self.cache_timestamp = None\n",
    "        self.cache_ttl = 60  # ç¼“å­˜60ç§’\n",
    "        \n",
    "    def get_gpu_resource_status(self, use_cache: bool = True) -> GPUResourceStatus:\n",
    "        \"\"\"è·å–é›†ç¾¤GPUèµ„æºçŠ¶æ€\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # æ£€æŸ¥ç¼“å­˜\n",
    "        if (use_cache and self.cache_timestamp and \n",
    "            current_time - self.cache_timestamp < self.cache_ttl):\n",
    "            logger.info(\"ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„GPUèµ„æºä¿¡æ¯\")\n",
    "            return self.gpu_cache.get('status')\n",
    "            \n",
    "        logger.info(\"ğŸ” æ­£åœ¨æŸ¥è¯¢é›†ç¾¤GPUèµ„æºçŠ¶æ€...\")\n",
    "        \n",
    "        try:\n",
    "            nodes = self.k8s_manager.v1_core.list_node()\n",
    "            gpu_nodes = []\n",
    "            total_gpus = 0\n",
    "            available_gpus = 0\n",
    "            \n",
    "            for node in nodes.items:\n",
    "                gpu_info = self._analyze_gpu_node(node)\n",
    "                if gpu_info and gpu_info.gpu_count > 0:\n",
    "                    gpu_nodes.append(gpu_info)\n",
    "                    total_gpus += gpu_info.gpu_count\n",
    "                    available_gpus += gpu_info.available_gpus\n",
    "                    \n",
    "            status = GPUResourceStatus(\n",
    "                total_gpus=total_gpus,\n",
    "                available_gpus=available_gpus,\n",
    "                used_gpus=total_gpus - available_gpus,\n",
    "                gpu_nodes=gpu_nodes\n",
    "            )\n",
    "            \n",
    "            # æ›´æ–°ç¼“å­˜\n",
    "            self.gpu_cache['status'] = status\n",
    "            self.cache_timestamp = current_time\n",
    "            \n",
    "            logger.info(f\"âœ… GPUèµ„æºç»Ÿè®¡: æ€»è®¡{total_gpus}å¡, å¯ç”¨{available_gpus}å¡, å·²ç”¨{total_gpus-available_gpus}å¡\")\n",
    "            \n",
    "            return status\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ æŸ¥è¯¢GPUèµ„æºå¤±è´¥: {e}\")\n",
    "            return GPUResourceStatus(0, 0, 0, [])\n",
    "    \n",
    "    def _analyze_gpu_node(self, node) -> Optional[GPUNodeInfo]:\n",
    "        \"\"\"åˆ†æå•ä¸ªèŠ‚ç‚¹çš„GPUä¿¡æ¯\"\"\"\n",
    "        node_name = node.metadata.name\n",
    "        labels = node.metadata.labels or {}\n",
    "        \n",
    "        # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å¯è°ƒåº¦\n",
    "        schedulable = not (node.spec.unschedulable or False)\n",
    "        \n",
    "        # è§£ææ±¡ç‚¹\n",
    "        taints = []\n",
    "        if node.spec.taints:\n",
    "            for taint in node.spec.taints:\n",
    "                taints.append({\n",
    "                    \"key\": taint.key,\n",
    "                    \"value\": taint.value or \"\",\n",
    "                    \"effect\": taint.effect\n",
    "                })\n",
    "        \n",
    "        # æ£€æŸ¥GPUèµ„æº\n",
    "        capacity = node.status.capacity or {}\n",
    "        allocatable = node.status.allocatable or {}\n",
    "        \n",
    "        gpu_count = 0\n",
    "        gpu_type = \"unknown\"\n",
    "        gpu_memory = \"unknown\"\n",
    "        \n",
    "        # æ£€æŸ¥NVIDIA GPU\n",
    "        if self.k8s_manager.config.GPU_VENDOR_NVIDIA in capacity:\n",
    "            gpu_count = int(capacity[self.k8s_manager.config.GPU_VENDOR_NVIDIA])\n",
    "            gpu_type = self._extract_gpu_type_from_labels(labels, \"nvidia\")\n",
    "            \n",
    "        # æ£€æŸ¥AMD GPU\n",
    "        elif self.k8s_manager.config.GPU_VENDOR_AMD in capacity:\n",
    "            gpu_count = int(capacity[self.k8s_manager.config.GPU_VENDOR_AMD])\n",
    "            gpu_type = self._extract_gpu_type_from_labels(labels, \"amd\")\n",
    "        \n",
    "        if gpu_count == 0:\n",
    "            return None\n",
    "            \n",
    "        # è®¡ç®—å¯ç”¨GPUæ•°é‡\n",
    "        available_gpus = self._calculate_available_gpus(node_name, gpu_count)\n",
    "        \n",
    "        return GPUNodeInfo(\n",
    "            name=node_name,\n",
    "            gpu_count=gpu_count,\n",
    "            gpu_type=gpu_type,\n",
    "            gpu_memory=gpu_memory,\n",
    "            available_gpus=available_gpus,\n",
    "            labels=labels,\n",
    "            taints=taints,\n",
    "            schedulable=schedulable\n",
    "        )\n",
    "    \n",
    "    def _extract_gpu_type_from_labels(self, labels: Dict[str, str], vendor: str) -> str:\n",
    "        \"\"\"ä»èŠ‚ç‚¹æ ‡ç­¾æå–GPUå‹å·\"\"\"\n",
    "        gpu_type_keys = [\n",
    "            f\"{vendor}.com/gpu-type\",\n",
    "            f\"{vendor}.com/gpu.product\",\n",
    "            \"accelerator\",\n",
    "            \"gpu-type\",\n",
    "            \"gpu.type\"\n",
    "        ]\n",
    "        \n",
    "        for key in gpu_type_keys:\n",
    "            if key in labels:\n",
    "                return labels[key]\n",
    "                \n",
    "        # é€šç”¨GPUå‹å·æ£€æµ‹\n",
    "        for key, value in labels.items():\n",
    "            if \"gpu\" in key.lower() and any(model in value.lower() \n",
    "                for model in [\"rtx\", \"gtx\", \"tesla\", \"a100\", \"v100\", \"t4\"]):\n",
    "                return value\n",
    "                \n",
    "        return f\"{vendor}-gpu\"\n",
    "    \n",
    "    def _calculate_available_gpus(self, node_name: str, total_gpus: int) -> int:\n",
    "        \"\"\"è®¡ç®—èŠ‚ç‚¹å¯ç”¨GPUæ•°é‡\"\"\"\n",
    "        try:\n",
    "            # æŸ¥è¯¢èŠ‚ç‚¹ä¸Šçš„Podèµ„æºä½¿ç”¨æƒ…å†µ\n",
    "            pods = self.k8s_manager.v1_core.list_pod_for_all_namespaces(\n",
    "                field_selector=f\"spec.nodeName={node_name}\"\n",
    "            )\n",
    "            \n",
    "            used_gpus = 0\n",
    "            for pod in pods.items:\n",
    "                if pod.status.phase in [\"Running\", \"Pending\"]:\n",
    "                    for container in pod.spec.containers:\n",
    "                        if container.resources and container.resources.requests:\n",
    "                            gpu_request = container.resources.requests.get(\n",
    "                                self.k8s_manager.config.GPU_VENDOR_NVIDIA, \"0\"\n",
    "                            )\n",
    "                            if gpu_request != \"0\":\n",
    "                                used_gpus += int(gpu_request)\n",
    "                                \n",
    "            return max(0, total_gpus - used_gpus)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"è®¡ç®—èŠ‚ç‚¹ {node_name} å¯ç”¨GPUæ—¶å‡ºé”™: {e}\")\n",
    "            return total_gpus  # ä¿å®ˆä¼°è®¡ï¼Œè¿”å›æ€»æ•°\n",
    "    \n",
    "    def find_suitable_gpu_nodes(self, required_gpus: int = 1, \n",
    "                               gpu_type_preference: str = None) -> List[GPUNodeInfo]:\n",
    "        \"\"\"æŸ¥æ‰¾é€‚åˆçš„GPUèŠ‚ç‚¹\"\"\"\n",
    "        status = self.get_gpu_resource_status()\n",
    "        suitable_nodes = []\n",
    "        \n",
    "        for node in status.gpu_nodes:\n",
    "            # æ£€æŸ¥åŸºæœ¬æ¡ä»¶\n",
    "            if (node.schedulable and \n",
    "                node.available_gpus >= required_gpus):\n",
    "                \n",
    "                # æ£€æŸ¥GPUç±»å‹åå¥½\n",
    "                if gpu_type_preference:\n",
    "                    if gpu_type_preference.lower() in node.gpu_type.lower():\n",
    "                        suitable_nodes.append(node)\n",
    "                else:\n",
    "                    suitable_nodes.append(node)\n",
    "        \n",
    "        # æŒ‰å¯ç”¨GPUæ•°é‡é™åºæ’åº\n",
    "        suitable_nodes.sort(key=lambda x: x.available_gpus, reverse=True)\n",
    "        \n",
    "        logger.info(f\"ğŸ¯ æ‰¾åˆ° {len(suitable_nodes)} ä¸ªé€‚åˆçš„GPUèŠ‚ç‚¹\")\n",
    "        return suitable_nodes\n",
    "\n",
    "# åˆå§‹åŒ–GPUèµ„æºç®¡ç†å™¨\n",
    "gpu_manager = GPUResourceManager(k8s_manager)\n",
    "\n",
    "# æŸ¥è¯¢GPUèµ„æºçŠ¶æ€\n",
    "print(\"ğŸ” æ­£åœ¨æŸ¥è¯¢é›†ç¾¤GPUèµ„æº...\")\n",
    "gpu_status = gpu_manager.get_gpu_resource_status(use_cache=False)\n",
    "\n",
    "print(f\"\\nğŸ“Š GPUèµ„æºæ¦‚è§ˆ:\")\n",
    "print(f\"  æ€»GPUæ•°é‡: {gpu_status.total_gpus}\")\n",
    "print(f\"  å¯ç”¨GPU: {gpu_status.available_gpus}\")\n",
    "print(f\"  å·²ç”¨GPU: {gpu_status.used_gpus}\")\n",
    "print(f\"  GPUèŠ‚ç‚¹æ•°: {len(gpu_status.gpu_nodes)}\")\n",
    "\n",
    "print(f\"\\nğŸ–¥ï¸ GPUèŠ‚ç‚¹è¯¦æƒ…:\")\n",
    "for i, node in enumerate(gpu_status.gpu_nodes):\n",
    "    print(f\"  èŠ‚ç‚¹ {i+1}: {node.name}\")\n",
    "    print(f\"    GPUç±»å‹: {node.gpu_type}\")\n",
    "    print(f\"    GPUæ•°é‡: {node.gpu_count} (å¯ç”¨: {node.available_gpus})\")\n",
    "    print(f\"    å¯è°ƒåº¦: {node.schedulable}\")\n",
    "    print(f\"    æ±¡ç‚¹æ•°é‡: {len(node.taints)}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºé‡è¦æ ‡ç­¾\n",
    "    important_labels = {k: v for k, v in node.labels.items() \n",
    "                       if any(keyword in k.lower() for keyword in ['gpu', 'accelerator', 'instance-type'])}\n",
    "    if important_labels:\n",
    "        print(f\"    é‡è¦æ ‡ç­¾: {important_labels}\")\n",
    "    print()\n",
    "\n",
    "# æµ‹è¯•æŸ¥æ‰¾é€‚åˆçš„GPUèŠ‚ç‚¹\n",
    "if gpu_status.available_gpus > 0:\n",
    "    print(\"ğŸ¯ æµ‹è¯•æŸ¥æ‰¾é€‚åˆçš„GPUèŠ‚ç‚¹:\")\n",
    "    suitable_nodes = gpu_manager.find_suitable_gpu_nodes(required_gpus=1)\n",
    "    for node in suitable_nodes[:3]:  # æ˜¾ç¤ºå‰3ä¸ªæœ€ä½³èŠ‚ç‚¹\n",
    "        print(f\"  æ¨èèŠ‚ç‚¹: {node.name} (å¯ç”¨GPU: {node.available_gpus})\")\n",
    "else:\n",
    "    print(\"âš ï¸ å½“å‰æ²¡æœ‰å¯ç”¨çš„GPUèµ„æº\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624462af",
   "metadata": {},
   "source": [
    "## 5. Pythonè„šæœ¬è§£æå’Œé¢„å¤„ç†\n",
    "\n",
    "è§£æJupyterHubä¸­çš„Pythonè„šæœ¬ï¼Œæå–èµ„æºéœ€æ±‚ï¼Œè¯†åˆ«GPUä¾èµ–ï¼Œè¿›è¡Œè„šæœ¬éªŒè¯å’Œé¢„å¤„ç†ã€‚"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
