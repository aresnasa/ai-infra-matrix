# AI Infrastructure Matrix - 统一 Docker Compose 配置
# 版本: v3.0.0 - 自定义Nginx镜像版本 (嵌入式静态文件)
# 作者: AI Infrastructure Team

services:
  postgres:
    image: postgres:15-alpine
    container_name: ai-infra-postgres
    env_file:
    - .env
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      TZ: Asia/Shanghai
    expose:
      - "5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - ai-infra-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  # Redis 缓存服务
  redis:
    image: redis:7-alpine
    container_name: ai-infra-redis
    env_file:
    - .env
    # Defer expansion of REDIS_PASSWORD to the container environment
    command: ["sh", "-c", "exec redis-server --requirepass \"$${REDIS_PASSWORD}\""]
    expose:
      - "6379"
    volumes:
      - redis_data:/data
    networks:
      - ai-infra-network
    environment:
      TZ: Asia/Shanghai
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -a \"$${REDIS_PASSWORD}\" ping || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  # Kafka 消息队列服务 (KRaft模式，无需Zookeeper)
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: ai-infra-kafka
    environment:
      # KRaft 模式配置
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9094
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093,PLAINTEXT_HOST://0.0.0.0:9094
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      
      # 集群和主题配置
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      
      # 日志和存储配置
      KAFKA_LOG_RETENTION_HOURS: 168  # 7天
      KAFKA_LOG_SEGMENT_BYTES: 1073741824  # 1GB
      KAFKA_LOG_RETENTION_BYTES: 10737418240  # 10GB
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      
      # 性能优化
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 102400
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 102400
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 104857600
      KAFKA_NUM_NETWORK_THREADS: 8
      KAFKA_NUM_IO_THREADS: 8
      KAFKA_BACKGROUND_THREADS: 10
      
      # 集群元数据配置（KRaft）
      CLUSTER_ID: 'gYf__u4_TgSoREBUnP-YzQ'
      
      TZ: Asia/Shanghai
    expose:
      - "9092"
      - "9093"
    ports:
      - "9094:9094"  # 外部访问端口
    volumes:
      - kafka_data:/var/lib/kafka/data
      - kafka_logs:/var/log/kafka
    networks:
      - ai-infra-network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  # Kafka UI 管理界面
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: ai-infra-kafka-ui
    depends_on:
      kafka:
        condition: service_healthy
    volumes:
      - ./config/kafka-ui-config.yml:/etc/kafka-ui/dynamic_config.yml
    environment:
      DYNAMIC_CONFIG_ENABLED: 'true'
      TZ: Asia/Shanghai
    expose:
      - "8080"
    ports:
      - "9095:8080"  # Kafka UI 管理界面
    networks:
      - ai-infra-network
    restart: unless-stopped

  # OpenLDAP 目录服务
  openldap:
    image: osixia/openldap:stable
    container_name: ai-infra-openldap
    env_file:
    - .env
    environment:
      LDAP_ORGANISATION: "AI Infrastructure"
      LDAP_DOMAIN: "ai-infra.com"
      LDAP_ADMIN_PASSWORD: "${LDAP_ADMIN_PASSWORD}"
      LDAP_CONFIG_PASSWORD: "${LDAP_CONFIG_PASSWORD}"
      LDAP_BASE_DN: "dc=ai-infra,dc=com"
      TZ: Asia/Shanghai
    expose:
      - "389"
      - "636"
    volumes:
      - ldap_data:/var/lib/ldap
      - ldap_config:/etc/ldap/slapd.d
    networks:
      - ai-infra-network
    healthcheck:
      test: ["CMD", "sh", "-c", "ldapsearch -x -H ldap://localhost -b dc=ai-infra,dc=com -D cn=admin,dc=ai-infra,dc=com -w \"$$LDAP_ADMIN_PASSWORD\""]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  # phpLDAPadmin Web界面
  phpldapadmin:
    image: osixia/phpldapadmin:stable
    container_name: ai-infra-phpldapadmin
    environment:
      PHPLDAPADMIN_LDAP_HOSTS: openldap
      PHPLDAPADMIN_HTTPS: "false"
      TZ: Asia/Shanghai
    expose:
      - "80"
    depends_on:
      openldap:
        condition: service_healthy
    networks:
      - ai-infra-network
    restart: unless-stopped

  # 后端初始化服务 - 创建admin用户和基础数据
  backend-init:
    image: ai-infra-backend-init:${IMAGE_TAG:-v0.3.6-dev}
    build:
      context: ./src/backend
      dockerfile: Dockerfile
      target: backend-init
      args:
        VERSION: ${IMAGE_TAG:-v0.3.6-dev}
    container_name: ai-infra-backend-init
    env_file:
    - .env
    environment:
      - DB_HOST=${POSTGRES_HOST:-postgres}
      - DB_PORT=${POSTGRES_PORT:-5432}
      - DB_USER=${POSTGRES_USER:-postgres}
      - DB_PASSWORD=${POSTGRES_PASSWORD:-postgres}
      - DB_NAME=${POSTGRES_DB:-ansible_playbook_generator}
      - REDIS_HOST=${REDIS_HOST:-redis}
      - REDIS_PORT=${REDIS_PORT:-6379}
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - LDAP_SERVER=${LDAP_HOST:-openldap}
      - LDAP_PORT=${LDAP_PORT:-389}
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - TZ=Asia/Shanghai
    command: ["./init"]
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      openldap:
        condition: service_healthy
    networks:
      - ai-infra-network
    restart: "no"

  # 后端 API 服务
  backend:
    image: ai-infra-backend:${IMAGE_TAG:-v0.3.6-dev}
    build:
      context: ./src/backend
      dockerfile: Dockerfile
      target: backend
      args:
        VERSION: ${IMAGE_TAG:-v0.3.6-dev}
    container_name: ai-infra-backend
    env_file:
    - .env
    environment:
      DB_HOST: "${POSTGRES_HOST:-postgres}"
      DB_PORT: "${POSTGRES_PORT:-5432}"
      DB_USER: "${POSTGRES_USER:-postgres}"
      DB_PASSWORD: "${POSTGRES_PASSWORD:-postgres}"
      DB_NAME: "${POSTGRES_DB:-ansible_playbook_generator}"
      REDIS_HOST: "${REDIS_HOST:-redis}"
      REDIS_PORT: "${REDIS_PORT:-6379}"
      REDIS_PASSWORD: "${REDIS_PASSWORD}"
      LDAP_SERVER: "${LDAP_HOST:-openldap}"
      LDAP_PORT: "${LDAP_PORT:-389}"
      LDAP_BASE_DN: "${LDAP_BASE_DN:-dc=example,dc=org}"
      GITEA_ENABLED: "${GITEA_ENABLED:-true}"
      GITEA_BASE_URL: "${GITEA_BASE_URL:-http://gitea:3000}"
      GITEA_ADMIN_TOKEN: "${GITEA_ADMIN_TOKEN}"
      GITEA_AUTO_CREATE: "${GITEA_AUTO_CREATE:-true}"
      GITEA_AUTO_UPDATE: "${GITEA_AUTO_UPDATE:-true}"
      GITEA_SYNC_ENABLED: "${GITEA_SYNC_ENABLED:-true}"
      GITEA_SYNC_INTERVAL_SECONDS: "${GITEA_SYNC_INTERVAL_SECONDS:-600}"
      # Map reserved username 'admin' to a real admin account in Gitea (use 'admin' as default)
      GITEA_ALIAS_ADMIN_TO: "${GITEA_ALIAS_ADMIN_TO:-admin}"
      LOG_LEVEL: "${LOG_LEVEL:-info}"
      TZ: "Asia/Shanghai"
    expose:
      - "8082"
    extra_hosts:
      - "kubernetes.docker.internal:host-gateway"
      - "host.docker.internal:host-gateway"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      openldap:
        condition: service_healthy
      backend-init:
        condition: service_completed_successfully
    volumes:
      - ./src/backend/outputs:/app/outputs
      - ./src/backend/uploads:/app/uploads
    networks:
      - ai-infra-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8082/api/health"]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  # 前端应用服务
  frontend:
    image: ai-infra-frontend:${IMAGE_TAG:-v0.3.6-dev}
    build:
      context: ./src/frontend
      dockerfile: Dockerfile
      args:
        REACT_APP_API_URL: /api
        REACT_APP_JUPYTERHUB_URL: /jupyter
        VERSION: ${IMAGE_TAG:-v0.3.6-dev}
    container_name: ai-infra-frontend
    environment:
      TZ: Asia/Shanghai
    expose:
      - "80"
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - ai-infra-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # JupyterHub 统一认证服务
  jupyterhub:
    image: ai-infra-jupyterhub:${IMAGE_TAG:-v0.3.6-dev}
    env_file:
    - .env
    build:
      context: ./src/jupyterhub
      dockerfile: Dockerfile
      args:
        BUILDKIT_INLINE_CACHE: "1"
        VERSION: ${IMAGE_TAG:-v0.3.6-dev}
    container_name: ai-infra-jupyterhub
    environment:
      - POSTGRES_HOST=${POSTGRES_HOST:-postgres}
      - POSTGRES_PORT=${POSTGRES_PORT:-5432}
      - POSTGRES_DB=jupyterhub_db
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
      - DB_HOST=${POSTGRES_HOST:-postgres}
      - DB_PORT=${POSTGRES_PORT:-5432}
      - DB_NAME=jupyterhub_db
      - DB_USER=${POSTGRES_USER:-postgres}
      - DB_PASSWORD=${POSTGRES_PASSWORD:-postgres}
      - REDIS_HOST=${REDIS_HOST:-redis}
      - REDIS_PORT=${REDIS_PORT:-6379}
      - REDIS_DB=1
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - JWT_SECRET=${JWT_SECRET}
      - JUPYTERHUB_ADMIN_USERS=admin,jupyter-admin
      - CONFIGPROXY_AUTH_TOKEN=${CONFIGPROXY_AUTH_TOKEN}
      - JUPYTERHUB_CRYPT_KEY=${JUPYTERHUB_CRYPT_KEY:-a3d7c9e5b1f2048c7d9e3b6a5c1f08e2a7b3c9d5e1f2048c7d9e3b6a5c1f08e2}
      - SESSION_TIMEOUT=86400
      - USE_CUSTOM_AUTH=true
      - AI_INFRA_BACKEND_URL=${BACKEND_URL:-http://backend:8082}
      - AI_INFRA_API_TOKEN=ai-infra-hub-token
      - JUPYTERHUB_AUTO_LOGIN=true
      - JUPYTERHUB_DEV_MODE=true
      - JUPYTERHUB_IMAGE=ai-infra-singleuser:${IMAGE_TAG:-v0.3.6-dev}
      - JUPYTERHUB_NETWORK=ai-infra-network
      - JUPYTERHUB_MEM_LIMIT=3G
      - JUPYTERHUB_CPU_LIMIT=1.0
      - JUPYTERHUB_IDLE_CULLER_ENABLED=true
      - JUPYTERHUB_IDLE_TIMEOUT=3600
      - JUPYTERHUB_CULL_TIMEOUT=7200
      - JUPYTERHUB_DEBUG=false
      - JUPYTERHUB_LOG_LEVEL=INFO
      - JUPYTERHUB_ACCESS_LOG=true
      - JUPYTERHUB_USE_PROXY=true
      - JUPYTERHUB_PUBLIC_HOST=${JUPYTERHUB_PUBLIC_HOST}
      - JUPYTERHUB_CORS_ORIGIN=${JUPYTERHUB_CORS_ORIGIN}
      - TZ=Asia/Shanghai
    ports:
      - "8088:8000"
    expose:
      - "8000"
      - "8091"
    volumes:
      - jupyterhub_data:/srv/data/jupyterhub
      - jupyterhub_notebooks:/srv/jupyterhub/notebooks
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./shared:/srv/jupyterhub/shared:rw
      - ./src/jupyterhub/jupyterhub_config.py:/srv/jupyterhub/jupyterhub_config.py:ro
      - ./src/jupyterhub/backend_integrated_config.py:/srv/jupyterhub/backend_integrated_config.py:ro
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      backend:
        condition: service_healthy
    networks:
      - ai-infra-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/jupyter/hub/api"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  # 单用户镜像构建器（不运行，仅用于构建镜像供Spawner使用）
  singleuser-builder:
    image: ai-infra-singleuser:${IMAGE_TAG:-v0.3.6-dev}
    build:
      context: ./src/singleuser
      dockerfile: Dockerfile
      args:
        VERSION: ${IMAGE_TAG:-v0.3.6-dev}
    command: ["true"]
    networks:
      - ai-infra-network
    restart: "no"

  # SaltStack 配置管理服务
  saltstack:
    image: ai-infra-saltstack:${IMAGE_TAG:-v0.3.6-dev}
    build:
      context: ./src/saltstack
      dockerfile: Dockerfile
      args:
        VERSION: ${IMAGE_TAG:-v0.3.6-dev}
    container_name: ai-infra-saltstack
    env_file:
    - .env
    environment:
      - TZ=Asia/Shanghai
      - SALT_MASTER_HOST=saltstack
      - AI_INFRA_BACKEND_URL=${BACKEND_URL:-http://backend:8082}
      - DEBUG_MODE=${DEBUG_MODE:-false}
    expose:
      - "4505"  # Salt Master Publish Port
      - "4506"  # Salt Master Return Port
      - "8000"  # Salt API Port
    volumes:
      - salt_data:/var/cache/salt
      - salt_logs:/var/log/salt
      - salt_keys:/etc/salt/pki
    networks:
      - ai-infra-network
    depends_on:
      backend:
        condition: service_healthy
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "sh", "-c", "nc -z 127.0.0.1 4505 && nc -z 127.0.0.1 4506 && nc -z 127.0.0.1 8000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  # SLURM Operator Manager - SLURM集群管理控制器
  slurm-operator-manager:
    image: ai-infra-slurm-operator-manager:${IMAGE_TAG:-v0.3.6-dev}
    build:
      context: ./src/slurm-operator
      dockerfile: Dockerfile
      target: manager
      args:
        VERSION: ${IMAGE_TAG:-v0.3.6-dev}
    container_name: ai-infra-slurm-operator-manager
    env_file:
    - .env
    environment:
      - TZ=Asia/Shanghai
      - WATCH_NAMESPACE=${SLURM_WATCH_NAMESPACE:-ai-infra}
      - POD_NAME=slurm-operator-manager
      - OPERATOR_NAME=slurm-operator
      - ACCOUNTING_WORKERS=${SLURM_ACCOUNTING_WORKERS:-4}
      - CONTROLLER_WORKERS=${SLURM_CONTROLLER_WORKERS:-4}
      - LOGINSET_WORKERS=${SLURM_LOGINSET_WORKERS:-4}
      - NODESET_WORKERS=${SLURM_NODESET_WORKERS:-4}
      - RESTAPI_WORKERS=${SLURM_RESTAPI_WORKERS:-4}
      - TOKEN_WORKERS=${SLURM_TOKEN_WORKERS:-4}
      - SLURMCLIENT_WORKERS=${SLURM_SLURMCLIENT_WORKERS:-2}
      - LOG_LEVEL=${SLURM_LOG_LEVEL:-info}
      # Kubernetes 相关配置
      - KUBECONFIG=/etc/kubernetes/config
      - KUBERNETES_SERVICE_HOST=${KUBERNETES_SERVICE_HOST:-kubernetes.docker.internal}
      - KUBERNETES_SERVICE_PORT=${KUBERNETES_SERVICE_PORT:-6443}
    expose:
      - "9443"  # Webhook 端口
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - slurm_operator_data:/var/lib/slurm-operator
    extra_hosts:
      - "kubernetes.docker.internal:host-gateway"
      - "host.docker.internal:host-gateway"
    depends_on:
      backend:
        condition: service_healthy
      postgres:
        condition: service_healthy
    networks:
      - ai-infra-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  # SLURM Operator Webhook - SLURM集群验证Webhook
  slurm-operator-webhook:
    image: ai-infra-slurm-operator-webhook:${IMAGE_TAG:-v0.3.6-dev}
    build:
      context: ./src/slurm-operator
      dockerfile: Dockerfile
      target: webhook
      args:
        VERSION: ${IMAGE_TAG:-v0.3.6-dev}
    container_name: ai-infra-slurm-operator-webhook
    env_file:
    - .env
    environment:
      - TZ=Asia/Shanghai
      - POD_NAME=slurm-operator-webhook
      - OPERATOR_NAME=slurm-operator
      - WEBHOOK_TIMEOUT_SECONDS=${SLURM_WEBHOOK_TIMEOUT:-10}
      - LOG_LEVEL=${SLURM_WEBHOOK_LOG_LEVEL:-info}
      # Kubernetes 相关配置
      - KUBECONFIG=/etc/kubernetes/config
      - KUBERNETES_SERVICE_HOST=${KUBERNETES_SERVICE_HOST:-kubernetes.docker.internal}
      - KUBERNETES_SERVICE_PORT=${KUBERNETES_SERVICE_PORT:-6443}
    expose:
      - "9443"  # Webhook 服务端口
    volumes:
      - slurm_operator_webhook_data:/var/lib/slurm-operator-webhook
      - slurm_operator_certs:/etc/ssl/certs/slurm-operator
    extra_hosts:
      - "kubernetes.docker.internal:host-gateway"
      - "host.docker.internal:host-gateway"
    depends_on:
      slurm-operator-manager:
        condition: service_healthy
      backend:
        condition: service_healthy
    networks:
      - ai-infra-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  # Nginx 反向代理服务 - 自定义镜像版本
  nginx:
    image: ai-infra-nginx:${IMAGE_TAG:-v0.3.6-dev}
    build:
      context: .
      dockerfile: src/nginx/Dockerfile
      args:
        DEBUG_MODE: ${DEBUG_MODE:-false}
        BUILD_ENV: ${BUILD_ENV:-production}
        VERSION: ${IMAGE_TAG:-v0.3.6-dev}
    container_name: ai-infra-nginx
    ports:
      - "8080:80"
      - "8443:443"
      - "8001:8001"
    volumes:
      - nginx_logs:/var/log/nginx
    env_file:
    - .env
    environment:
      - DEBUG_MODE=${DEBUG_MODE:-false}
      - BUILD_ENV=${BUILD_ENV:-production}
      - BACKEND_HOST=${BACKEND_HOST:-backend}
      - BACKEND_PORT=${BACKEND_PORT:-8082}
      - FRONTEND_HOST=${FRONTEND_HOST:-frontend}
      - FRONTEND_PORT=${FRONTEND_PORT:-80}
      - JUPYTERHUB_HOST=${JUPYTERHUB_HOST:-jupyterhub}
      - JUPYTERHUB_PORT=${JUPYTERHUB_PORT:-8000}
      - TZ=Asia/Shanghai
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      openldap:
        condition: service_healthy
      minio:
        condition: service_healthy
      gitea:
        condition: service_healthy
      frontend:
        condition: service_healthy
      backend:
        condition: service_healthy
      jupyterhub:
        condition: service_healthy
      saltstack:
        condition: service_healthy
      slurm-operator-manager:
        condition: service_healthy
      slurm-operator-webhook:
        condition: service_healthy
    networks:
      - ai-infra-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 90s
    restart: unless-stopped

  # Kubernetes 代理服务 (可选)
  k8s-proxy:
    image: tecnativa/tcp-proxy
    container_name: ai-infra-k8s-proxy
    environment:
      LISTEN: "0.0.0.0:6443"
      TALK: "host.docker.internal:6443"
      PRE_RESOLVE: "0"
      VERBOSE: "1"
      TZ: Asia/Shanghai
    expose:
      - "6443"
    extra_hosts:
      - "kubernetes.docker.internal:host-gateway"
      - "host.docker.internal:host-gateway"
    networks:
      - ai-infra-network
    restart: unless-stopped

  # Redis 监控界面 (可选)
  redis-insight:
    image: redislabs/redisinsight:latest
    container_name: ai-infra-redis-insight
    environment:
      TZ: Asia/Shanghai
    expose:
      - "8001"
    depends_on:
      - redis
    networks:
      - ai-infra-network
    restart: unless-stopped

  # OpenSCOW 数据库 (MySQL)
  openscow-db:
    image: mysql:8
    container_name: ai-infra-openscow-db
    env_file:
    - .env
    environment:
      MYSQL_ROOT_PASSWORD: ${OPENSCOW_MYSQL_ROOT_PASSWORD:-openscowroot}
      MYSQL_DATABASE: scow
      MYSQL_USER: scow
      MYSQL_PASSWORD: ${OPENSCOW_MYSQL_PASSWORD:-openscowpass}
      TZ: Asia/Shanghai
    expose:
      - "3306"
    volumes:
      - openscow_db_data:/var/lib/mysql
    networks:
      - ai-infra-network
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "scow", "-p${OPENSCOW_MYSQL_PASSWORD:-openscowpass}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  # OpenSCOW Redis
  openscow-redis:
    image: redis:7-alpine
    container_name: ai-infra-openscow-redis
    env_file:
    - .env
    command: ["sh", "-c", "exec redis-server --requirepass \"$${OPENSCOW_REDIS_PASSWORD:-openscowredis}\" --appendonly yes"]
    expose:
      - "6379"
    volumes:
      - openscow_redis_data:/data
    networks:
      - ai-infra-network
    environment:
      TZ: Asia/Shanghai
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -a \"$${OPENSCOW_REDIS_PASSWORD:-openscowredis}\" ping || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  # OpenSCOW SSH 服务器
  openscow-ssh:
    image: alpine-ssh
    container_name: ai-infra-openscow-ssh
    build:
      context: ./src/openscow/dev/ssh-server
    env_file:
    - .env
    environment:
      TZ: Asia/Shanghai
    expose:
      - "22"
    volumes:
      - openscow_ssh_data:/home
      - openscow_ssh_keys:/etc/ssh/keys
    networks:
      - ai-infra-network
    restart: unless-stopped

  # OpenSCOW 主应用服务
  openscow:
    image: ai-infra-openscow:${IMAGE_TAG:-v0.3.6-dev}
    build:
      context: ./src/openscow
      dockerfile: docker/Dockerfile.scow
      args:
        VERSION: ${IMAGE_TAG:-v0.3.6-dev}
    container_name: ai-infra-openscow
    env_file:
    - .env
    environment:
      # 数据库配置
      DB_HOST: openscow-db
      DB_PORT: 3306
      DB_NAME: scow
      DB_USER: scow
      DB_PASSWORD: ${OPENSCOW_MYSQL_PASSWORD:-openscowpass}
      # Redis 配置
      REDIS_HOST: openscow-redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: ${OPENSCOW_REDIS_PASSWORD:-openscowredis}
      # SSH 配置
      SSH_HOST: openscow-ssh
      SSH_PORT: 22
      # 应用配置
      SCOW_LAUNCH_APP: portal-web
      NODE_ENV: production
      TZ: Asia/Shanghai
      # 外部访问配置
      BASE_PATH: /openscow
      PUBLIC_URL: ${DOMAIN:-localhost}/openscow
    expose:
      - "3000"  # 主应用端口
      - "80"    # Nginx 端口
      - "5000"  # API 端口
    volumes:
      - openscow_data:/app/data
      - openscow_logs:/app/logs
    depends_on:
      openscow-db:
        condition: service_healthy
      openscow-redis:
        condition: service_healthy
      openscow-ssh:
        condition: service_started
    networks:
      - ai-infra-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  # Gitea 代码托管服务（可选，供门户内嵌）
  gitea:
    image: ai-infra-gitea:${IMAGE_TAG:-v0.3.6-dev}
    build:
      context: ./src/gitea
      dockerfile: Dockerfile
    container_name: ai-infra-gitea
    env_file:
    - .env
    environment:
      USER_UID: "1000"
      USER_GID: "1000"
      ROOT_URL: "${ROOT_URL}"
      SUBURL: "/gitea"
      STATIC_URL_PREFIX: "/gitea"
      GITEA__server__ROOT_URL: "${ROOT_URL}"
      GITEA__server__SUBURL: "/gitea"
      DOMAIN: "${DOMAIN}"
      PROTOCOL: "http"
      HTTP_PORT: "3000"
      GITEA_DB_TYPE: "postgres"
      GITEA_DB_HOST: "${GITEA_DB_HOST:-postgres:5432}"
      GITEA_DB_NAME: "${GITEA_DB_NAME:-gitea}"
      # 注意：以下两行必须使用6个空格缩进，保持与其他 environment 项对齐，否则会导致 YAML 解析错误。
      GITEA_DB_USER: "${GITEA_DB_USER:-gitea}"
      GITEA_DB_PASSWD: "${GITEA_DB_PASSWD:-gitea-password}"
      POSTGRES_USER: "${POSTGRES_USER:-postgres}"
      POSTGRES_PASSWORD: "${POSTGRES_PASSWORD:-postgres}"
      DISABLE_REGISTRATION: "false"
      REVERSE_PROXY_TRUSTED_PROXIES: "0.0.0.0/0,::/0"
      GITEA__auth__REVERSE_PROXY_AUTHENTICATION_USER: "X-WEBAUTH-USER"
      GITEA__auth__REVERSE_PROXY_AUTHENTICATION_EMAIL: "X-WEBAUTH-EMAIL"
      GITEA__auth__REVERSE_PROXY_AUTHENTICATION_FULL_NAME: "X-WEBAUTH-FULLNAME"
      GITEA__auth__PROXY_ENABLED: "true"
      GITEA__auth__PROXY_HEADER_NAME: "X-WEBAUTH-USER"
      GITEA__auth__PROXY_EMAIL_HEADER: "X-WEBAUTH-EMAIL"
      GITEA__auth__PROXY_FULL_NAME_HEADER: "X-WEBAUTH-FULLNAME"
      GITEA__security__REVERSE_PROXY_LIMIT: "1"
      GITEA__security__REVERSE_PROXY_TRUSTED_PROXIES: "0.0.0.0/0,::/0"
      # Set initial admin identity to 'admin' user
      INITIAL_ADMIN_USERNAME: "${GITEA_ALIAS_ADMIN_TO:-admin}"
      # Admin bootstrap via reverse-proxy SSO only; admin user will be default admin
      GITEA__storage__STORAGE_TYPE: "${GITEA_STORAGE:-local}"
      DATA_PATH: "/data/gitea"
      MINIO_ENDPOINT: "${MINIO_HOST:-minio}:${MINIO_PORT:-9000}"
      MINIO_BUCKET: "gitea"
      MINIO_USE_SSL: "false"
      MINIO_LOCATION: "us-east-1"
      MINIO_ACCESS_KEY: "${MINIO_ACCESS_KEY:-minioadmin}"
      MINIO_SECRET_KEY: "${MINIO_SECRET_KEY:-minioadmin}"
      TZ: "Asia/Shanghai"
    expose:
      - "3000"
      - "22"
    ports:
      # Debug: expose internal 3000 on host 3010 for direct backend access (bypass Nginx)
      - "3010:3000"
    volumes:
      - gitea_data:/data
    depends_on:
      postgres:
        condition: service_healthy
      backend:
        condition: service_healthy
    networks:
      - ai-infra-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/"]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  # MinIO (S3-compatible)
  minio:
    image: minio/minio:latest
    container_name: ai-infra-minio
    command: server /data --console-address ":9001"
    env_file:
    - .env
    environment:
      - MINIO_ROOT_USER=${MINIO_ACCESS_KEY:-minioadmin}
      - MINIO_ROOT_PASSWORD=${MINIO_SECRET_KEY:-minioadmin}
      - TZ=Asia/Shanghai
    expose:
      - "9000"
      - "9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - ai-infra-network
    restart: unless-stopped

volumes:
  postgres_data:
    name: ai-infra-postgres-data
  redis_data:
    name: ai-infra-redis-data
  kafka_data:
    name: ai-infra-kafka-data
  kafka_logs:
    name: ai-infra-kafka-logs
  ldap_data:
    name: ai-infra-ldap-data
  ldap_config:
    name: ai-infra-ldap-config
  jupyterhub_data:
    name: ai-infra-jupyterhub-data
  jupyterhub_notebooks:
    name: ai-infra-jupyterhub-notebooks
  nginx_logs:
    name: ai-infra-nginx-logs
  salt_data:
    name: ai-infra-salt-data
  salt_logs:
    name: ai-infra-salt-logs
  salt_keys:
    name: ai-infra-salt-keys
  slurm_operator_data:
    name: ai-infra-slurm-operator-data
  slurm_operator_webhook_data:
    name: ai-infra-slurm-operator-webhook-data
  slurm_operator_certs:
    name: ai-infra-slurm-operator-certs
  gitea_data:
    name: ai-infra-gitea-data
  minio_data:
    name: ai-infra-minio-data

networks:
  ai-infra-network:
    name: ai-infra-network
    driver: bridge
